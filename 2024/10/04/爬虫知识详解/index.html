<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <meta name="theme-color" content="#3367D6"/>
  <link rel="apple-touch-icon" href="/icons-192.png">
  <link rel="manifest" href="/manifest.json">
  
  <meta name="generator" content="Hexo 7.3.0">

  
    <meta name="description" content="Zhou">
  

  

  
    <meta name="author" content="ChengZhou">
  

  

  

  <title>爬虫知识详解 | XiaoZhou</title>

  

  
    <link rel="shortcut icon" href="/favicon.ico">
  

  

  

  

  
<link rel="stylesheet" href="/css/style.css">

</head>
<body>
  <div class="root-container">
    
<!-- header container -->
<header class="header-container post">
  
    <div class="post-image" style="background-image: url(https://qiniu.sukoshi.xyz/src/images/68686407_p0.jpg)"></div>
  

  <!-- navbar -->
<nav class="navbar">
  <div class="navbar-content">
    <!-- logo -->
    <div class="navbar-logo">
      <a href="/">
        
          XiaoZhou
        
      </a>
    </div>
    <!-- link -->
    <div class="navbar-link">
      <div class="navbar-btn">
        <div></div>
        <div></div>
        <div></div>
      </div>
      <ul class="navbar-list">
        
          <li class="navbar-list-item"><a href="/">首页</a></li>
        
          <li class="navbar-list-item"><a href="/archives">归档</a></li>
        
          <li class="navbar-list-item"><a href="/links">友链</a></li>
        
          <li class="navbar-list-item"><a href="/about">关于</a></li>
        
      </ul>
    </div>
  </div>
</nav>

  
  

  
  

  
  

  
  

  
  
    <div class="header-content">
      <div class="post-text layout-block">
        <div class="layout-margin">
          <h1 class="title-wrap">爬虫知识详解</h1>
          <h2 class="title-sub-wrap">
            <strong>ChengZhou</strong>
            <span>发布于</span>
            <time  class="article-date" datetime="2024-10-04T13:04:39.000Z" itemprop="datePublished">2024-10-04</time>
          </h2>
          
          
          <ul class="wrap-list dark">
  
</ul>
          <ul class="wrap-list dark">
  
    <li><a href="/tags/%E7%88%AC%E8%99%AB/">🏷️ 爬虫</a></li>
  
</ul>
        </div>
      </div>
    </div>
  

  
  
  
</header>

    <!-- 文章 -->

<!-- 文章内容 -->
<div class="body-container">
  <article class="content-container layout-block post-container">
    <div class="article-info">
      
      
      
      
      <section class="article-entry markdown-body layout-margin content-padding--large soft-size--large soft-style--box">
        <h1><span id="前言">前言</span></h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">复制#!/usr/bin/env python</span><br><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line"># 以上两行代码表示声明解释器位置，以及本文件用utf-8编码</span><br><span class="line"># 设置方式：</span><br><span class="line"># File -&gt; Settings -&gt; Editor -&gt; File and Code Templates -&gt; Python Script -&gt; 右侧设置默认文本</span><br><span class="line"></span><br><span class="line">import urllib3</span><br><span class="line">urllib3.disable_warnings()</span><br><span class="line"># 消除不必要的警告，和编码错误，这是前置条件不可少</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h1><span id="一-抓包工具">一、抓包工具</span></h1><h2><span id="一-原理">一、原理</span></h2><blockquote>
<p>这类型工具的原理都是通过代理进行抓包的</p>
</blockquote>
<ul>
<li>http就是简单的显示，然后转发</li>
<li>https因为有加密，所以抓包工具没办法直接显示，需要通过安装抓包工具的证书，再客户端和服务器之间增增加一层抓包工具。<br>对于浏览器来说，抓包工具就是服务器，应用抓包工具的证书进行加解密；对于服务器来说，抓包工具就是客户端，应用服务器的真实证书进行加解密。</li>
</ul>
<h2><span id="二-抓包工具的作用">二、抓包工具的作用</span></h2><ul>
<li>能够监听数据</li>
<li>能够正常显示数据</li>
<li>能够设置过滤条件，查看特定目标数据，过滤host，协议，类型等等</li>
<li>能够在监听到的数据中进行查找</li>
<li>能够中断请求，修改参数，手动提交</li>
<li>能够自动提交请求，测试服务器限制</li>
</ul>
<h2><span id="三-准备工作">三、准备工作</span></h2><h3><span id="1-浏览器">1、浏览器</span></h3><blockquote>
<p>需要安装代理工具，各个浏览器不同，总之就是可以快速应用代理配置，比系统代理设置简单易用<br>需要启用插件，chrome勾选启用并且勾选开发者模式</p>
</blockquote>
<h3><span id="2-防火墙">2、防火墙</span></h3><blockquote>
<p>关闭系统防火墙或第三方防火墙，或者设置允许规则</p>
</blockquote>
<h3><span id="3-清除系统代理和ie代理">3、清除系统代理和IE代理</span></h3><h2><span id="四-charles">四、charles</span></h2><ul>
<li>1、安装证书，双击证书或在cmd中输入certmgr.msc</li>
<li>2、配置http和https，proxy setting ， SSL proxying setting</li>
<li>3、proxy&gt;&gt;recording setting ，进行过滤</li>
<li>3、断点设置，可以观察每一步操作</li>
<li>4、edit，可以编辑后发送</li>
<li>5、repeat和repeat advance</li>
<li>6、模拟慢速网络</li>
<li>7、filter和recording setting</li>
<li>8、map和rewrite</li>
</ul>
<p>注意事项：</p>
<ul>
<li>不建议进行host过滤捕捉，有可能遗漏请求</li>
<li>显示乱码，查看raw</li>
<li>显示正方形框，需要去浏览器查看</li>
<li>依照抓包工具的分析编写爬虫流程后，设置代理，捕获自己的爬虫程序的数据库，和正常访问的对比</li>
</ul>
<h2><span id="五-fiddler">五、fiddler</span></h2><h3><span id="1-捕获窗口表格的列说明">1、捕获窗口表格的列说明</span></h3><blockquote>
</blockquote>
<ul>
<li>#：抓取HTTP Request的序号，从1开始，以此递增</li>
<li>Result：HTTP状态码</li>
<li>Protocol：请求使用的协议，如HTTP&#x2F;HTTPS&#x2F;FTP等</li>
<li>Host：请求地址的主机名</li>
<li>URL：请求资源的位置</li>
<li>Body：该请求的大小</li>
<li>Caching:请求的缓存过期时间或者缓存控制值</li>
<li>Content-Type:请求响应的类型</li>
<li>Process:发送此请求的进程：进程ID</li>
<li>Comments:允许用户为此回话添加备注</li>
<li>Custom：允许用户设置自定义值</li>
</ul>
<h3><span id="2-内置命令与断点命令">2、内置命令与断点命令</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">复制命令 |对应请求项|   介绍                                      | 示例  </span><br><span class="line">内置命令：</span><br><span class="line">?      All       问号后边跟一个字符串，可以匹配出包含这个字符串的请求        ?google</span><br><span class="line">&gt;      Body      大于号后面跟一个数字，可以匹配出请求大小，大于这个数字请求	  &gt;1000</span><br><span class="line">&lt;      Body      小于号跟大于号相反，匹配出请求大小，小于这个数字的请求	      &lt;100</span><br><span class="line">=      Result    等于号后面跟数字，可以匹配HTTP返回码	                      =200</span><br><span class="line">@      Host      @后面跟Host，可以匹配域名				             @www.baidu.com</span><br><span class="line">select Content-Type   select后面跟响应类型，可以匹配到相关的类型	      select image</span><br><span class="line">cls    All       清空当前所有请求			                              cls</span><br><span class="line">dump   All       将所有请求打包成saz压缩包，保存到“我的文档\Fiddler2\Captures”目录下 dump	</span><br><span class="line">start  All       开始监听请求				                              start</span><br><span class="line">stop   All       停止监听请求	                                          stop</span><br><span class="line"></span><br><span class="line">断点命令 </span><br><span class="line">bpafter All      bpafter后边跟一个字符串，表示中断所有包含该字符串的请求	  bpafter baidu（输入bpafter解除断点）</span><br><span class="line">bpu     All      跟bpafter差不多，只不过这个是收到请求了，中断响应	      bpu baidu（输入bpu解除断点）</span><br><span class="line">bps     Result   后面跟状态吗，表示中断所有是这个状态码的请求	          bps 200（输入bps解除断点）</span><br><span class="line">bpv/bpm HTTP方法 只中断HTTP方法的命令，HTTP方法如POST、GET	              bpv get（输入bpv解除断点）</span><br><span class="line">g/go    All      放行所有中断下来的请求                                 g</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3><span id="3-websocket">3、websocket</span></h3><h1><span id="二-get-post-session基础用法">二、GET、POST、session基础用法</span></h1><h2><span id="一-get-option-head等方法">一、get、option、head等方法</span></h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">### 1、参数</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<ul>
<li>url ： 目标网站的地址</li>
<li>params: url的参数部分，接受的是字典 或 ((‘k1’,’v1’),(‘k2’,’v2’)) ， 没参数可以忽略</li>
<li>headers: 请求的headers部分，字典类型</li>
<li>cookies: 指定的cookies，一般由requests自己控制</li>
<li>files: 文件上传时，指定的参数</li>
<li>auth: auth效验，一般不使用</li>
<li>timeout: 2种参数，一种是 数字，就是所有的超时， 这种使用最多！一种是 (connect timeout, read timeout) ，2个数值的元组，第一个参数是连接超时的时间，第二个参数是 读取response超时时间</li>
<li>allow_redirects: 302 类似这样的状态，是否允许自动跳转，默认是允许</li>
<li>proxies: 代理参数， {‘http’:’127.0.0.1:8888’}</li>
<li>verify: 是否效验https服务器的证书是否安全，默认是True，但是我们一般都会修改为 False</li>
<li>stream: 流方式获取内容，一般不会使用</li>
<li>cert: 第一种方式： (.pem)， 指定本地文件夹中的 pem文件的完整路径</li>
</ul>
<p> 第二种方式： (‘test.cert’, ‘test.key’) ，就是cert文件和key文件的完整路径</p>
<h3><span id="2-基本请求">2、基本请求</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">复制r = requests.get(&#x27;http://httpbin.org/get&#x27;)</span><br><span class="line"></span><br><span class="line">print(r.content)  # content是获取 bytes</span><br><span class="line"></span><br><span class="line">print(r.text)  # text 是获取str</span><br><span class="line"></span><br><span class="line">print(r.status_code)  # status_code 状态码</span><br><span class="line"></span><br><span class="line">print(r.headers)  # 获取响应的头内容，字典</span><br><span class="line"></span><br><span class="line">print(r.cookies)  # 获取响应的 cookie 内容 ，RequestsCookieJar 类似字典</span><br><span class="line"></span><br><span class="line">print(r.encoding) # 获取响应的 编码</span><br><span class="line"></span><br><span class="line">print(r.url) # 获取响应的 url 地址</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2><span id="二-post方法">二、POST方法</span></h2><h3><span id="1-参数">1、参数</span></h3><ul>
<li><em>url ： 目标网站的地址</em></li>
<li><em>params: url的参数部分，接受的是字典 或 ((‘k1’,’v1’),(‘k2’,’v2’)) ， 没参数可以忽略</em></li>
<li><em>data: form表单数据的提交，在request的body部分，形式是 urlencode ， k1&#x3D;v1&amp;k2&#x3D;v2 ， 和json不同时提交</em></li>
<li><em>json: json数据提交，在body部分是： {‘k1’:’v2’} ，和 data不同时提交</em></li>
<li><em>headers: 请求的headers部分，字典类型</em></li>
<li><em>cookies: 指定的cookies，一般由requests自己控制</em></li>
<li><em>files: 文件上传时，指定的参数</em></li>
<li><em>auth: auth效验，一般不使用</em></li>
<li><em>timeout: 2种参数，一种是 数字，就是所有的超时， 这种使用最多！</em></li>
</ul>
<p> <em>一种是 (connect timeout, read timeout) ，2个数值的元组，第一个参数是连接超时的时间，第二个参数是 读取response超时时间</em></p>
<ul>
<li><em>allow_redirects: 302 类似这样的状态，是否允许自动跳转，默认是允许</em></li>
<li><em>proxies: 代理参数， {‘http’:’127.0.0.1:8888’}</em></li>
<li><em>verify: 是否效验https服务器的证书是否安全，默认是True，但是我们一般都会修改为 False</em></li>
<li><em>stream: 流方式获取内容，一般不会使用</em></li>
<li><em>cert: 第一种方式： (.pem)， 指定本地文件夹中的 pem文件的完整路径</em></li>
</ul>
<p> <em>第二种方式： (‘test.cert’, ‘test.key’) ，就是cert文件和key文件的完整路径</em></p>
<h2><span id="三-session用法">三、session用法</span></h2><h3><span id="1-标准的session初始化">1、标准的session初始化</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">复制# 使用session，可以统一设置一些共有的参数，而不需要在每一步的get或post中设置，可继承属性</span><br><span class="line">s = requests.session()</span><br><span class="line">s.verify = False # 这个不可少，不然访问https请求，会报错</span><br><span class="line">s.trust_env = False # 不是系统自带的配置，代理配置、verify配置</span><br><span class="line"># s.proxies = &#123;&#x27;http&#x27;: &#x27;127.0.0.1:8888&#x27; , &#x27;https&#x27;: &#x27;127.0.0.1:8888&#x27;&#125;</span><br><span class="line">s.headers = &#123;</span><br><span class="line">    &#x27;header1&#x27;: &#x27;v1&#x27;,</span><br><span class="line">    &#x27;header2&#x27;: &#x27;v2&#x27;,</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h1><span id="三-编码">三、编码</span></h1><h2><span id="一-前言">一、前言</span></h2><blockquote>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">requests 的 response的编码：</span><br><span class="line">在 requests.utils 中的 get_encoding_from_headers 方法 ，进行的编码判断</span><br><span class="line">1、response的headers中，设置了 content-type， </span><br><span class="line">    其中必须是包含 text ,但是没有设置 charset，那么response的 encoding会设置为  ISO-8859-1</span><br><span class="line">2、response的headers中，设置了 content-type， </span><br><span class="line">    值包含text，并且设置了charset，那么 encoding 就是 这个 charset 的值</span><br><span class="line">3、headers中没有设置 content-type，那么 encoding 就是  UTF-8</span><br><span class="line"></span><br><span class="line">eg:    r.encoding = &#x27;utf-8&#x27;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
</blockquote>
<h2><span id="二-常见编码">二、常见编码</span></h2><h1><span id="四-一般流程">四、一般流程</span></h1><h2><span id="一-步骤">一、步骤</span></h2><h3><span id="1-抓包">1、抓包</span></h3><ul>
<li>1、清除浏览器缓存</li>
<li>2、有隐身窗口的使用隐身窗口，没有的就打开一个新的标签</li>
<li>3、打开 charles，并且监听</li>
<li>4、浏览器中启用代理</li>
<li>5、在浏览器中输入目标网址，如： <em><a target="_blank" rel="noopener" href="https://www.qidian.com/rank/yuepiao">https://www.qidian.com/rank/yuepiao</a></em></li>
<li>6、在浏览器正常操作，查看到所有需要的信息的网页</li>
<li>7、操作完成后，停止charles的抓包，关闭网页的代理</li>
<li>8、清理charles的抓包数据</li>
<li>9、保存抓包信息</li>
</ul>
<h1><span id="五-算法加密">五、算法加密</span></h1><p>(原理就是对bs64进行加密)</p>
<h2><span id="一-常见算法加密">一、常见算法加密</span></h2><h3><span id="1-md5-rsa-des-3des">1、MD5、RSA、DES、3DES</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">复制任何加密都是针对的bytes： b&#x27;\xB3\X3B&#x27;</span><br><span class="line">URL编码解码：</span><br><span class="line">  用于url的参数提交</span><br><span class="line">  中文、特殊字符 转换为 %B3%3B%53。。。。。。</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">复制base64：</span><br><span class="line">    是网络上最常见的用于传输8Bit字节码的编码方式之一，</span><br><span class="line">    Base64就是一种基于64个可打印字符来表示二进制数据的方法，用于在HTTP环境下传递较长的标识信息</span><br><span class="line">    后一两位可能有“=”</span><br><span class="line">    防君子不防小人，很容易解密</span><br><span class="line"></span><br><span class="line">    输出为 A-Z、a-z、0-9和&quot;+&quot;、&quot;/&quot; 字符组成的字符串</span><br><span class="line">    在urlencode中：  _   -   替换  +   / </span><br><span class="line">    很多时候字符串尾部为 1个或2个  &quot;=&quot;</span><br><span class="line">    把3个字节的二进制拼接， 24位， 按6位分割，变成4个字节，每个字节小于64</span><br><span class="line">    最后留下1个字节的时候，会在尾部添加 2个 &#x27;=&#x27;</span><br><span class="line">    最后留下2个字节的时候，会在尾部添加 1个 &#x27;=&#x27;</span><br><span class="line">    </span><br><span class="line">    &#x27;中&#x27; 这个汉字 ， utf-8是：  b&#x27;\xe4\xb8\xad&#x27;</span><br><span class="line">    转为 二进制：  1110 01 00 1011 1000 10 10 1101</span><br><span class="line">    按6个位进行分割4个字节：   111001  001011 100010 101101</span><br><span class="line">    在前面补0 :    00111001  00001011  00100010  00101101</span><br><span class="line">    </span><br><span class="line">    4个字节:  b&#x27;\xe4\xb8\xad\x48&#x27;</span><br><span class="line">    转为 二进制:      1110 01 00 1011 1000 10 10 1101  0100 1000</span><br><span class="line">    按6个位进行分割4个字节： 111001  001011 100010 101101   010010  00</span><br><span class="line">    前面4个字节处理掉，剩余尾部： 010010  00</span><br><span class="line">    第二个会在前面补0，依然只有2个字节: 010010  00000000  </span><br><span class="line">    转换为这个字符串的时候，假设前面2个是A0，会自动在尾部不组4个字符，即补2个 &#x27;==&#x27;： A0==</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">复制MD5:</span><br><span class="line">    Message-Digest Algorithm 5（摘要算法5）</span><br><span class="line">    1、压缩性：任意长度的数据，算出的MD5值长度都是固定的。</span><br><span class="line">    2、容易计算：从原数据计算出MD5值很容易。</span><br><span class="line">    3、抗修改性：对原数据进行任何改动，哪怕只修改1个字节，所得到的MD5值都有很大区别。</span><br><span class="line">    4、强抗碰撞：已知原数据和其MD5值，想找到一个具有相同MD5值的数据（即伪造数据）是非常困难的。</span><br><span class="line"></span><br><span class="line">    输出为 128 bit，  每4位二进制组合一个十六进制字符，一般输出为 长度 32 个16进制字符串</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">复制DES:</span><br><span class="line">    全称为Data Encryption Standard，即数据加密标准，是一种使用密钥加密的块算法</span><br><span class="line">    入口参数有三个：Key、Data、Mode</span><br><span class="line">    Key为7个字节共56位，是DES算法的工作密钥；</span><br><span class="line">    Data为8个字节64位，是要被加密或被解密的数据；</span><br><span class="line">    Mode为DES的工作方式,有两种:加密或解密</span><br><span class="line"></span><br><span class="line">    3DES（即Triple DES）是DES向AES过渡的加密算法，</span><br><span class="line">    使用两个密钥，执行三次DES算法，</span><br><span class="line">    加密的过程是加密-解密-加密</span><br><span class="line">    解密的过程是解密-加密-解密</span><br><span class="line"></span><br><span class="line">    pycrypto安装指南：帮助文档（https://www.dlitz.net/software/pycrypto/api/current/）</span><br><span class="line">    要先安装VC2015：microsoft visual studio 2015(14)</span><br><span class="line">    1、http://blog.csdn.net/a624806998/article/details/78596543</span><br><span class="line">        在执行 python setup.py install 之前，运行</span><br><span class="line">        set CL=/FI&quot;%VCINSTALLDIR%\\INCLUDE\\stdint.h&quot; %CL%</span><br><span class="line">    2、出现ImportError: No module named &#x27;winrandom&#x27;错误</span><br><span class="line">        处理：修改python3安装目录下的 lib/Crypto/Random/OSRNG/nt.py 文件中找到</span><br><span class="line">        import winrandom</span><br><span class="line">        修改为</span><br><span class="line">        from Crypto.Random.OSRNG import winrandom</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">复制AES：</span><br><span class="line">    高级加密标准（英语：Advanced Encryption Standard，缩写：AES），这个标准用来替代原先的DES</span><br><span class="line">    AES的区块长度固定为128 比特，密钥长度则可以是128，192或256比特 （16、24和32字节）</span><br><span class="line">    大致步骤如下：</span><br><span class="line">    1、密钥扩展（KeyExpansion），</span><br><span class="line">    2、初始轮（Initial Round），</span><br><span class="line">    3、重复轮（Rounds），每一轮又包括：SubBytes、ShiftRows、MixColumns、AddRoundKey，</span><br><span class="line">    4、最终轮（Final Round），最终轮没有MixColumns。</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">复制RSA:</span><br><span class="line">    公钥加密算法，一种非对称密码算法</span><br><span class="line">    公钥加密，私钥解密</span><br><span class="line"></span><br><span class="line">    3个参数：</span><br><span class="line">    rsa_n， rsa_e，message</span><br><span class="line">    rsa_n, rsa_e  用于生成公钥</span><br><span class="line">    message： 需要加密的消息</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">复制还要很多网站有自定义的加密算法，不是通用的算法，处理方式：</span><br><span class="line">    1、破解js，写对应的python算法。优点：执行快，缺点：复杂，难度高，有可能随时需要更新</span><br><span class="line">    2、selenium 进行浏览器模拟</span><br><span class="line">    3、pyexec，下载这个JS，用pyexec调用这个js的方法</span><br><span class="line"></span><br><span class="line">    有一些参数很复杂，但是你可以尝试不提交，就是提交 &quot;&quot;，是有可能通过的</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2><span id="二-常见加密形式">二、常见加密形式</span></h2><h3><span id="1-遇到号">1、遇到’%’号</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">复制parse.quote(s) # 编码</span><br><span class="line">parse.unquote(s) # 解码</span><br><span class="line">urlencode(d)   #  将字典转为get参数串</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h1><span id="六-文本解析语言">六、文本解析语言</span></h1><h2><span id="一-xpath">一、Xpath</span></h2><h3><span id="1-常用节点解析">1、常用节点解析</span></h3><ul>
<li><p>nodename 选取此节点的所有子节点。</p>
<p>&#x2F; 从根节点选取。<br>&#x2F;&#x2F; 从匹配选择的当前节点选择文档中的节点，而不考虑它们的位置。<br>. 选取当前节点。<br>.. 选取当前节点的父节点。</p>
</li>
<li><p>@ 选取属性</p>
</li>
<li><p><em>: 匹配任何元素节点。@</em> 匹配任何属性节点。</p>
</li>
<li><p>node() 配任何类型的节点</p>
</li>
<li><p>and 多属性连接 ，或者 | : p[@class&#x3D;’v1’ and @type&#x3D;’hidden’] , ‘&#x2F;&#x2F;p | &#x2F;&#x2F;b’</p>
</li>
<li><p>text() a[text()&#x3D;’test’]，文本等于</p>
<p>PS：xpath得到的值都是list，需要通过下标来进行获取元素对象</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">在 xpath 中获取多个元素的下标，是从1开始，譬如： //div/p[1] 就是指div元素下的第一个p元素</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3><span id="2-实用案列">2、实用案列</span></h3></li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">复制# lxml为必备库需要下载</span><br><span class="line">from lxml import etree</span><br><span class="line"># lxml初始化，方式一</span><br><span class="line">tree = etree.HTML(html_doc) # 会自动补全 html 标签</span><br><span class="line">print(etree.tostring(tree, encoding=&quot;utf-8&quot;).decode(&#x27;utf-8&#x27;))</span><br><span class="line"># lxml初始化，方式二</span><br><span class="line">tree = etree.fromstring(html_doc)</span><br><span class="line">print(etree.tostring(tree, encoding=&quot;utf-8&quot;).decode(&#x27;utf-8&#x27;))</span><br><span class="line"># lxml初始化，方式三</span><br><span class="line">tree = etree.XML(html_doc)</span><br><span class="line">print(etree.tostring(tree, encoding=&quot;utf-8&quot;).decode(&#x27;utf-8&#x27;))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 以下为实例</span><br><span class="line">tree = etree.HTML(html_doc)</span><br><span class="line">print(tree.xpath(&#x27;//title&#x27;)[0].text) # 节点内容</span><br><span class="line">print(tree.xpath(&#x27;//title&#x27;)[0].tag)  # 节点名</span><br><span class="line">print(tree.xpath(&#x27;//p[@class=&quot;story&quot;]&#x27;)[1].text)</span><br><span class="line"></span><br><span class="line">print(etree.tostring(tree.xpath(&#x27;//title&#x27;)[0]))  # etree.tostring 输出节点全部信息</span><br><span class="line"></span><br><span class="line">print(tree.xpath(&#x27;//title&#x27;)[0].getparent().tag)  # 父节点</span><br><span class="line"></span><br><span class="line">print(tree.xpath(&#x27;//a&#x27;)[1].get(&#x27;class&#x27;))  # 获取属性</span><br><span class="line">print(tree.xpath(&#x27;//a&#x27;)[1].attrib)  # 所有属性的字典</span><br><span class="line"></span><br><span class="line">print(tree.xpath(&quot;//text()&quot;)) # 所有字符串，列表形式</span><br><span class="line">print(tree.xpath(&quot;//text()&quot;)[2]) # 所有字符串，列表形式</span><br><span class="line"></span><br><span class="line">print(tree.xpath(&quot;string()&quot;)) # 所有文本，字符串 类型，以单一标签为分界，如 &lt;br/&gt;</span><br><span class="line"></span><br><span class="line">多属性</span><br><span class="line">print(tree.xpath(&#x27;//a[@class=&quot;sister&quot; and @id=&quot;link2&quot;]&#x27;)[0].text)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(tree.xpath(&#x27;//a[@class=&quot;sister&quot; and contains(text(), &quot;Tillie&quot;)]&#x27;)[0].text)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(tree.xpath(&#x27;//a[@class=&quot;sister&quot; and text()=&quot;Tillie&quot;]&#x27;)[0].text)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2><span id="二-正则split">二、正则split</span></h2><h2><span id="1-基础语法">1、基础语法</span></h2><p><a target="_blank" rel="noopener" href="https://www.runoob.com/regexp/regexp-syntax.html">基础语法连接一</a></p>
<p><a target="_blank" rel="noopener" href="https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Guide/Regular_Expressions">基础语法连接二</a></p>
<table>
<thead>
<tr>
<th>字符</th>
<th>描述</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td>\</td>
<td>将下一个字符标记为一个特殊字符、或一个原义字符、或一个 向后引用、或一个八进制转义符。例如，’n’ 匹配字符 “n”。’\n’ 匹配一个换行符。序列 ‘\’ 匹配 “\” 而 “(“ 则匹配 “(“。</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>^</td>
<td>匹配输入字符串的开始位置。如果设置了 <strong>RegExp</strong> 对象的 <strong>Multiline</strong> 属性，^ 也匹配 ‘\n’ 或 ‘\r’ 之后的位置。</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>$</td>
<td>匹配输入字符串的结束位置。如果设置了<strong>RegExp</strong> 对象的 <strong>Multiline</strong> 属性，$ 也匹配 ‘\n’ 或 ‘\r’ 之前的位置。</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>*</td>
<td>匹配前面的子表达式零次或多次。例如，zo 能匹配 “z” 以及 “zoo”。 <em>等价于{0,}。</em></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>+</td>
<td>匹配前面的子表达式一次或多次。例如，’zo+’ 能匹配 “zo” 以及 “zoo”，但不能匹配 “z”。+ 等价于 {1,}。</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>?</td>
<td>匹配前面的子表达式零次或一次。例如，”do(es)?” 可以匹配 “do” 或 “does” 中的”do” 。? 等价于 {0,1}。</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>{<em>n</em>}</td>
<td><em>n</em> 是一个非负整数。匹配确定的 <em>n</em> 次。例如，’o{2}’ 不能匹配 “Bob” 中的 ‘o’，但是能匹配 “food” 中的两个 o。</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>{<em>n</em>,}</td>
<td><em>n</em> 是一个非负整数。至少匹配<em>n</em> 次。例如，’o{2,}’ 不能匹配 “Bob” 中的 ‘o’，但能匹配 “foooood” 中的所有 o。’o{1,}’ 等价于 ‘o+’。’o{0,}’ 则等价于 ‘o’。</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>{<em>n</em>,<em>m</em>}</td>
<td><em>m</em> 和 <em>n</em> 均为非负整数，其中<em>n</em> &lt;&#x3D; <em>m</em>。最少匹配 <em>n</em> 次且最多匹配 <em>m</em> 次。例如，”o{1,3}” 将匹配 “fooooood” 中的前三个 o。’o{0,1}’ 等价于 ‘o?’。请注意在逗号和两个数之间不能有空格。</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>?</td>
<td>当该字符紧跟在任何一个其他限制符 (<em>, +, ?, {n}, {n,}, {n,m}) 后面时，匹配模式是非贪婪的。非贪婪模式尽可能少的匹配所搜索的字符串，而默认的贪婪模式则尽可能多的匹配所搜索的字符串。例如，对于字符串 “oooo”，’o+?’ 将匹配单个 “o”，而 ‘o+’ 将匹配所有 ‘o’。</em></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>.</td>
<td>匹配除 “\n” 之外的任何单个字符。要匹配包括 ‘\n’ 在内的任何字符，请使用象 ‘[.\n]’ 的模式。</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>(<em>pattern</em>)</td>
<td>匹配 <em>pattern</em> 并获取这一匹配。所获取的匹配可以从产生的 Matches 集合得到，在VBScript 中使用 <strong>SubMatches</strong> 集合，在JScript 中则使用 <strong>$0</strong>…**$9** 属性。要匹配圆括号字符，请使用 ‘(‘ 或 ‘)’。</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>(?:<em>pattern</em>)</td>
<td>匹配 <em>pattern</em> 但不获取匹配结果，也就是说这是一个非获取匹配，不进行存储供以后使用。这在使用 “或” 字符 (\</td>
<td>) 来组合一个模式的各个部分是很有用。例如， ‘industr(?:y\</td>
<td>ies) 就是一个比 ‘industry\</td>
<td>industries’ 更简略的表达式。</td>
</tr>
<tr>
<td>(?&#x3D;<em>pattern</em>)</td>
<td>正向预查，在任何匹配 <em>pattern</em> 的字符串开始处匹配查找字符串。这是一个非获取匹配，也就是说，该匹配不需要获取供以后使用。例如，’Windows (?&#x3D;95\</td>
<td>98\</td>
<td>NT\</td>
<td>2000)’ 能匹配 “Windows 2000” 中的 “Windows” ，但不能匹配 “Windows 3.1” 中的 “Windows”。预查不消耗字符，也就是说，在一个匹配发生后，在最后一次匹配之后立即开始下一次匹配的搜索，而不是从包含预查的字符之后开始。</td>
</tr>
<tr>
<td>(?!<em>pattern</em>)</td>
<td>负向预查，在任何不匹配 <em>pattern</em> 的字符串开始处匹配查找字符串。这是一个非获取匹配，也就是说，该匹配不需要获取供以后使用。例如’Windows (?!95\</td>
<td>98\</td>
<td>NT\</td>
<td>2000)’ 能匹配 “Windows 3.1” 中的 “Windows”，但不能匹配 “Windows 2000” 中的 “Windows”。预查不消耗字符，也就是说，在一个匹配发生后，在最后一次匹配之后立即开始下一次匹配的搜索，而不是从包含预查的字符之后开始</td>
</tr>
<tr>
<td><em>x</em>\</td>
<td><em>y</em></td>
<td>匹配 <em>x</em> 或 <em>y</em>。例如，’z\</td>
<td>food’ 能匹配 “z” 或 “food”。’(z\</td>
<td>f)ood’ 则匹配 “zood” 或 “food”。</td>
</tr>
<tr>
<td>[<em>xyz</em>]</td>
<td>字符集合。匹配所包含的任意一个字符。例如， ‘[abc]’ 可以匹配 “plain” 中的 ‘a’。</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>[^<em>xyz</em>]</td>
<td>负值字符集合。匹配未包含的任意字符。例如， ‘[^abc]’ 可以匹配 “plain” 中的’p’。</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>[<em>a-z</em>]</td>
<td>字符范围。匹配指定范围内的任意字符。例如，’[a-z]’ 可以匹配 ‘a’ 到 ‘z’ 范围内的任意小写字母字符。</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>[^<em>a-z</em>]</td>
<td>负值字符范围。匹配任何不在指定范围内的任意字符。例如，’[^a-z]’ 可以匹配任何不在 ‘a’ 到 ‘z’ 范围内的任意字符。</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>\b</td>
<td>匹配一个单词边界，也就是指单词和空格间的位置。例如， ‘er’ 可以匹配”never” 中的 ‘er’，但不能匹配 “verb” 中的 ‘er’。</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>\B</td>
<td>匹配非单词边界。’er\B’ 能匹配 “verb” 中的 ‘er’，但不能匹配 “never” 中的 ‘er’。</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>\c<em>x</em></td>
<td>匹配由 <em>x</em> 指明的控制字符。例如， \cM 匹配一个 Control-M 或回车符。<em>x</em> 的值必须为 A-Z 或 a-z 之一。否则，将 c 视为一个原义的 ‘c’ 字符。</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>\d</td>
<td>匹配一个数字字符。等价于 [0-9]。</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>\D</td>
<td>匹配一个非数字字符。等价于 [^0-9]。</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>\f</td>
<td>匹配一个换页符。等价于 \x0c 和 \cL。</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>\n</td>
<td>匹配一个换行符。等价于 \x0a 和 \cJ。</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>\r</td>
<td>匹配一个回车符。等价于 \x0d 和 \cM。</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>\s</td>
<td>匹配任何空白字符，包括空格、制表符、换页符等等。等价于 [ \f\n\r\t\v]。</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>\S</td>
<td>匹配任何非空白字符。等价于 [^ \f\n\r\t\v]。</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>\t</td>
<td>匹配一个制表符。等价于 \x09 和 \cI。</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>\v</td>
<td>匹配一个垂直制表符。等价于 \x0b 和 \cK。</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>\w</td>
<td>匹配包括下划线的任何单词字符。等价于’[A-Za-z0-9*]’。*</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>\W</td>
<td>匹配任何非单词字符。等价于 ‘[^A-Za-z0-9]’。</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>\x<em>n</em></td>
<td>匹配 <em>n</em>，其中 <em>n</em> 为十六进制转义值。十六进制转义值必须为确定的两个数字长。例如，’\x41’ 匹配 “A”。’\x041’ 则等价于 ‘\x04’ &amp; “1”。正则表达式中可以使用 ASCII 编码。.</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><em>num</em></td>
<td>匹配 <em>num</em>，其中 <em>num</em> 是一个正整数。对所获取的匹配的引用。例如，’(.)\1’ 匹配两个连续的相同字符。</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><em>n</em></td>
<td>标识一个八进制转义值或一个向后引用。如果 <em>n</em> 之前至少 <em>n</em> 个获取的子表达式，则 <em>n</em> 为向后引用。否则，如果 <em>n</em> 为八进制数字 (0-7)，则 <em>n</em> 为一个八进制转义值。</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><em>nm</em></td>
<td>标识一个八进制转义值或一个向后引用。如果 <em>nm</em> 之前至少有 <em>nm</em> 个获得子表达式，则 <em>nm</em> 为向后引用。如果 *nm* 之前至少有 <em>n</em> 个获取，则 <em>n</em> 为一个后跟文字 <em>m</em> 的向后引用。如果前面的条件都不满足，若<em>n</em> 和 <em>m</em> 均为八进制数字 (0-7)，则 *nm* 将匹配八进制转义值 <em>nm</em>。</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><em>nml</em></td>
<td>如果 <em>n</em> 为八进制数字 (0-3)，且 <em>m</em> 和 <em>l</em> 均为八进制数字 (0-7)，则匹配八进制转义值 <em>nml。</em></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>\u<em>n</em></td>
<td>匹配 <em>n</em>，其中 <em>n</em> 是一个用四个十六进制数字表示的 Unicode 字符。例如， \u00A9 匹配版权符号 (©)。</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody></table>
<h3><span id="2-实例用法">2、实例用法</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">复制book_auth = re.search(r&#x27;class=&quot;book-info &quot;.*?&lt;a.*?&gt;(.*?)&lt;/a&gt;&#x27;, text, re.RegexFlag.S|re.RegexFlag.M).group(1)</span><br><span class="line"># search(): 对需要的数据进行匹配，匹配到第一个就返回，匹配不到返回None</span><br><span class="line"></span><br><span class="line">urls = re.findall(r&#x27;class=&quot;book-mid-info&quot;.*?href=&quot;(.*?)&quot;.*?target=&quot;_blank&quot;&#x27;, text, re.RegexFlag.S | re.RegexFlag.M)</span><br><span class="line"># findall(): 获取匹配到的所有数据并返回。返回值是一个列表，不需要使用group()</span><br><span class="line"></span><br><span class="line">sub(): 将匹配到的数据进行替换</span><br><span class="line">用法：re.sub(r&quot;content&quot;,“data”,“text”)</span><br><span class="line">content: 正则表达式</span><br><span class="line">data: 正则匹配后需要替换的内容</span><br><span class="line">text: 需要匹配的内容</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2><span id="三-beautifulsoup4简称bs4">三、beautifulsoup4简称BS4</span></h2><h3><span id="1-简介">1、简介</span></h3><blockquote>
<p>python用于解析文本得库，支持第三方的解析器，譬如lxml、html5lib</p>
</blockquote>
<h3><span id="2-用法">2、用法</span></h3><h4><span id="1-初始化">1、初始化</span></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">复制soup = BeautifulSoup(html_doc, features)</span><br><span class="line"># features有11种：</span><br><span class="line"># fast, html, html.parser, html5, html5lib, lxml, lxml-html, lxml-xml, permissive, strict, xml</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<ul>
<li>bs4.builder._lxml.LXMLTreeBuilderForXML：xml、lxml-xml、也支持fast、xml、permissive</li>
<li>bs4.builder._lxml.LXMLTreeBuilder：默认、lxml、fast、html、lxml-html、permissive # 使用最多的对象</li>
<li>bs4.builder._html5lib.HTML5TreeBuilder：html5、html5lib， 也支持html、permissive</li>
<li>bs4.builder._htmlparser.HTMLParserTreeBuilder：strict、html.parser， 也支持html</li>
<li>TreeBuilder，4个类继承</li>
<li>HTMLTreeBuilder, 继承TreeBuilder，并由LXMLTreeBuilder、HTML5TreeBuilder和HTMLParserTreeBuilder继承</li>
<li>bs4.builder._htmlparser.BeautifulSoupHTMLParser ，继承系统的html.parser.HTMLParser，HTMLParserTreeBuilder 最终也是使用的这个类进行解析</li>
</ul>
<h4><span id="2-差异性">2、差异性</span></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">复制- 如果被解析的HTML文档是标准格式,那么解析器之间没有任何差别</span><br><span class="line">- 文档不规范时，会各自不同的处理</span><br><span class="line"></span><br><span class="line">	from bs4 import BeautifulSoup</span><br><span class="line">	text = &#x27;&lt;a&gt;&lt;b /&gt;&lt;/a&gt;&#x27;</span><br><span class="line">	soup_test = BeautifulSoup(text, &#x27;lxml&#x27;) # 会补全 html，body和p</span><br><span class="line">	print(&#x27;lxml:&#x27;)</span><br><span class="line">	print(soup_test)</span><br><span class="line">	soup_test = BeautifulSoup(text, &#x27;html.parser&#x27;) # 会补全 p</span><br><span class="line">	print(&#x27;html.parser:&#x27;)</span><br><span class="line">	print(soup_test)</span><br><span class="line">	soup_test = BeautifulSoup(text, &#x27;html5&#x27;)  # 会补全 html，head，body和p</span><br><span class="line">	print(&#x27;html5:&#x27;)</span><br><span class="line">	print(soup_test)</span><br><span class="line">	soup_test = BeautifulSoup(text, &#x27;xml&#x27;)  # 会加xml头，忽略错误的标签</span><br><span class="line">	print(&#x27;xml:&#x27;)</span><br><span class="line">	print(soup_test)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4><span id="3-详细用法">3、详细用法</span></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line">复制soup = BeautifulSoup(html_doc, &#x27;lxml&#x27;)</span><br><span class="line"></span><br><span class="line"># print(soup.prettify())</span><br><span class="line"></span><br><span class="line">print(soup.title) # 标签，包括标签本身</span><br><span class="line">print(soup.title.name) # 标签的名字</span><br><span class="line">s = soup.title.string</span><br><span class="line">print(soup.title.string) # 标签的内容, NavigableString 对象</span><br><span class="line">print(soup.title.text) # 标签的内容， str 对象</span><br><span class="line"></span><br><span class="line">print(soup.meta) # 标签</span><br><span class="line">print(soup.meta[&#x27;charset&#x27;]) # 标签属性</span><br><span class="line"></span><br><span class="line">print(soup.meta.parent.name) # 标签的父标签</span><br><span class="line">print(soup.html.parent.name)</span><br><span class="line">print(soup.html.parent.parent)</span><br><span class="line"></span><br><span class="line">body = soup.find(&#x27;body&#x27;)</span><br><span class="line">print(body.find(&#x27;div&#x27;))</span><br><span class="line"></span><br><span class="line">&#x27;&#x27;&#x27;</span><br><span class="line">    标签对象，可以和字符串一样编码和解码</span><br><span class="line">&#x27;&#x27;&#x27;</span><br><span class="line">markup = &quot;&lt;b&gt;\N&#123;SNOWMAN&#125;&lt;/b&gt;&quot;</span><br><span class="line">snowman_soup = BeautifulSoup(markup, &#x27;html.parser&#x27;)</span><br><span class="line">tag = snowman_soup.b</span><br><span class="line">print(tag)</span><br><span class="line">print(tag.encode(&quot;utf-8&quot;))</span><br><span class="line">print(tag.encode(&quot;utf-8&quot;).decode(&#x27;utf-8&#x27;))</span><br><span class="line">print(tag.encode(&quot;iso-8859-1&quot;))</span><br><span class="line">print(tag.encode(&quot;iso-8859-1&quot;).decode(&#x27;iso-8859-1&#x27;))</span><br><span class="line">print(tag.encode(&quot;gbk&quot;))</span><br><span class="line">print(tag.encode(&quot;gbk&quot;).decode(&#x27;gbk&#x27;))</span><br><span class="line"></span><br><span class="line">text = &#x27;&#x27;&#x27;</span><br><span class="line">&lt;a&gt;&lt;b&gt;text1&lt;/b&gt;&lt;c&gt;text2&lt;/c&gt;</span><br><span class="line">&lt;d&gt;text3&lt;/d&gt;&lt;e e1=&#x27;100&#x27;/&gt;&lt;f f1=&#x27;101&#x27;/&gt;&lt;&gt;&lt;/a&gt;</span><br><span class="line">&#x27;&#x27;&#x27;</span><br><span class="line">sibling_soup = BeautifulSoup(text, &#x27;lxml&#x27;)</span><br><span class="line">print(sibling_soup.b.next_sibling) #  兄弟节点</span><br><span class="line">print(sibling_soup.c.previous_sibling) #  兄弟节点</span><br><span class="line">print(sibling_soup.c.next_sibling) #  兄弟节点，是 换行符</span><br><span class="line">print(sibling_soup.d.previous_sibling) #  兄弟节点，是 换行符</span><br><span class="line"></span><br><span class="line">print(sibling_soup.a.next_element) #  下一个元素，是 &lt;b&gt;text1&lt;/b&gt;</span><br><span class="line">print(sibling_soup.b.next_element) #  下一个元素，是 text1</span><br><span class="line">print(sibling_soup.b.next_element.next_element) #  下一个元素，是 换行符</span><br><span class="line">print(sibling_soup.d.previous_element) #  上一个元素，是 换行符</span><br><span class="line">print(sibling_soup.f.previous_element) #  上一个元素，是 &lt;e e1=&#x27;100&#x27;/&gt;</span><br><span class="line">print(&#x27;结束&#x27;)</span><br><span class="line"></span><br><span class="line">soup = BeautifulSoup(html_doc, &#x27;lxml&#x27;)</span><br><span class="line">print(soup.find_all(&#x27;meta&#x27;)) # 查找所有</span><br><span class="line">print(soup.find_all(&#x27;meta&#x27;, limit=2)) # 查找所有</span><br><span class="line">print(soup.find(&#x27;meta&#x27;, &#123;&#x27;charset&#x27;: &#x27;UTF-8&#x27;&#125;)) # 查找特定的一个标签，其实也是调用的find_all，不过会在取到一个值后返回</span><br><span class="line">print(soup.find(id=&quot;seajsnode&quot;)) # 根据id查找特定的一个标签</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(soup.find(&#x27;p&#x27;, &#123;&quot;text&quot;: &#x27;测试的连接&#x27;&#125;))</span><br><span class="line">print(soup.find(&#x27;p&#x27;, text=&#x27;测试的连接&#x27;)) # 根据标签内容查找特定的一个标签，不能仅仅有标签内容一个参数</span><br><span class="line"></span><br><span class="line">meta = soup.find(&#x27;meta&#x27;, &#123;&#x27;name&#x27;: &#x27;robots&#x27;&#125;)</span><br><span class="line">print(meta)</span><br><span class="line">print(meta.find_next_sibling(&#x27;meta&#x27;)) # 查找下个符合条件的兄弟节点</span><br><span class="line">print(meta.find_next_siblings(&#x27;meta&#x27;)) # 查找所有符合条件的兄弟节点</span><br><span class="line">#</span><br><span class="line">print(meta.find_next_sibling(&#x27;a&#x27;)) # 查找下个符合条件的兄弟节点</span><br><span class="line">print(meta.find_next(&#x27;a&#x27;)) # 查找下个符合条件的节点</span><br><span class="line">print(meta.find_all_next(&#x27;a&#x27;)) # 查找所有符合条件的节点</span><br><span class="line"></span><br><span class="line">print(soup.find(&#x27;body&#x27;).get_text()) # 获取所有文本</span><br><span class="line">print(soup.find(&#x27;body&#x27;).get_text(&#x27;|&#x27;)) # 获取所有文本，| 是分隔符</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3><span id="3-注意事项">3、注意事项</span></h3><ul>
<li>bs4会自动替换 html_doc 中的 <meta charset="”gb2312”"> 为 <meta charset="”utf-8”">，这是html5的写法，html4之前是：</li>
<li>标签未结束的，会自动补全，例如： 会补全为 <meta charset="”gb2312”"></li>
</ul>
<h3><span id="4-beautifulsoup4-和其他方法的差异">4、beautifulsoup4 和其他方法的差异</span></h3><ul>
<li><p>beautifulsoup4 支持CSS选择器、Python标准库中的HTML解析器，也支持 lxml 的 XML解析器(css选择器，但是不支持xpath)，但是基于 DOM 的，会解析整个DOM树 ，全文档查询，</p>
</li>
<li><p>lxml使用xpath，则只会局部遍历</p>
</li>
<li><p>正则表达式，速度最快，但是最复杂，获取js中当中的数据一般使用正则</p>
<h5><span id="实例">实例：</span></h5><blockquote>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">xpath :  xpath(&quot;//p[@attr1=&quot;&quot;]/div/a&quot;).text</span><br><span class="line">获取 太初 这个小说名字：</span><br><span class="line">css选择器：soup.select(&#x27;div.book-info &gt; h1 &gt; em&#x27;)[0].text</span><br><span class="line"></span><br><span class="line">xpath： /html/body/div[2]/div[6]/div[1]/div[2]/h1/em</span><br><span class="line"></span><br><span class="line">正则表达式： re.search(r&#x27;class=&quot;book-info &quot;.*?&lt;em&gt;(.*?)&lt;/em&gt;&#x27;, html_doc, re.S).group(1)</span><br><span class="line"></span><br><span class="line">bs4： soup.find(&#x27;div&#x27;, &#123;&quot;class&quot;: &quot;book-info &quot;&#125;).h1.em.text</span><br><span class="line"></span><br></pre></td></tr></table></figure></blockquote>
</li>
</ul>
<h2><span id="四-css选择器">四、css选择器</span></h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">复制print(soup.select(&quot;title&quot;)) # 标签名</span><br><span class="line">print(soup.select(&quot;html head title&quot;)) # 逐层查找</span><br><span class="line">print(soup.select(&quot;body a&quot;)) # 不逐层查找</span><br><span class="line"></span><br><span class="line">print(soup.select(&quot;body &gt; a&quot;)) # &gt;  子节点</span><br><span class="line">print(len(soup.select(&quot;body &gt; div&quot;))) # &gt;  子节点</span><br><span class="line">print(soup.select(&quot;body &gt; div&quot;)) # &gt;  子节点</span><br><span class="line"></span><br><span class="line">print(soup.select(&quot;input ~ p&quot;)) # &gt;  兄弟节点</span><br><span class="line"></span><br><span class="line">print(soup.select(&quot;#pin-nav&quot;))  # 通过id</span><br><span class="line">print(soup.select(&quot;div#pin-nav&quot;))  # 通过id</span><br><span class="line"></span><br><span class="line">print(soup.select(&#x27;.share-img&#x27;)) # 通过class</span><br><span class="line"></span><br><span class="line">print(soup.select(&#x27;meta[charset=&quot;gb2312&quot;]&#x27;))</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h1><span id="七-selenium自动化工具">七、selenium自动化工具</span></h1><h2><span id="一-介绍重点下载selenium时一定要关闭所有代理">一、介绍（重点！！！下载selenium时一定要关闭所有代理）</span></h2><blockquote>
<p>一个自动化测试工具。<br>在爬虫中能做什么？</p>
<p>（多用于登录一些难以登录的网页以获取对应的cookies）</p>
<p>官网的说法：<br>Selenium automates browsers. That’s it! What you do with that power is entirely up to you.<br>浏览器的自动化操作，你想干嘛就干嘛…..就这么简单！</p>
<p>开源易用，支持多种语言<br>支持大部分主流的浏览器：firefox，chrome，ie，edge，safari，opera，phantomjs等等</p>
<p>官方文档：<a target="_blank" rel="noopener" href="https://www.seleniumhq.org/doc">https://www.seleniumhq.org/doc</a></p>
</blockquote>
<h2><span id="二-selenium使用">二、selenium使用</span></h2><blockquote>
<p>selenium包括了很多方面，如Selenium IDE 、Selenium Remote Control 、<br>Selenium Grid 、Selenium WebDriver<br>爬虫系统主要使用到Selenium WebDriver，可以在本地或远程计算机上驱动浏览器</p>
</blockquote>
<h2><span id="三-具体使用">三、具体使用</span></h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br></pre></td><td class="code"><pre><span class="line">复制# 基础用法</span><br><span class="line"># 浏览器的位置，相对路径，使用绝对路径也是可以的</span><br><span class="line">phantomjs_driver_path = &#x27;browser/phantomjs.exe&#x27;</span><br><span class="line"></span><br><span class="line">from selenium import webdriver</span><br><span class="line"></span><br><span class="line"># 启动驱动，不同的浏览器启用不同的类</span><br><span class="line">driver = webdriver.PhantomJS(phantomjs_driver_path)</span><br><span class="line"># driver = webdriver.Chrome(phantomjs_driver_path)</span><br><span class="line"></span><br><span class="line"># 设置窗口大小</span><br><span class="line">driver.set_window_size(1366, 768)</span><br><span class="line"># 页面的加载超时时间</span><br><span class="line">driver.set_page_load_timeout(10)</span><br><span class="line"># script脚本的超时时间</span><br><span class="line">driver.set_script_timeout(10)</span><br><span class="line"></span><br><span class="line"># 访问目标页面</span><br><span class="line">driver.get(&#x27;https://www.baidu.com&#x27;)</span><br><span class="line"></span><br><span class="line"># 下面有3种延时方式的展示，一般实际项目中不会同一个地方用3个延时，选择一个或多个使用</span><br><span class="line"></span><br><span class="line"># 绝对延时，等待规定时间后，直接执行后面的代码</span><br><span class="line">time.sleep(1)</span><br><span class="line"></span><br><span class="line"># 隐性延时，最长是30秒，如果30秒内，资源全部加载完成，那么执行后续的代码，</span><br><span class="line"># 30秒内没有加载完成，也会继续执行后续代码</span><br><span class="line">driver.implicitly_wait(30)</span><br><span class="line"></span><br><span class="line"># 显性等待，等待时长20秒，间隔0.5秒去查询一次，目标元素是否加载完成</span><br><span class="line"># 20秒内加载完成后，执行后续的代码，最长等待20秒，没有加载也会继续执行</span><br><span class="line"># from selenium.webdriver.support.wait import WebDriverWait</span><br><span class="line"># from selenium.webdriver.support import expected_conditions as EC</span><br><span class="line"># driver.get(&#x27;https://huilansame.github.io&#x27;)</span><br><span class="line"># locator = (By.LINK_TEXT, &#x27;CSDN&#x27;)</span><br><span class="line"># # 20 秒是最长等待时间，  0.5 秒是间隔轮询时间</span><br><span class="line"># WebDriverWait(driver, 20, 0.5).until(EC.presence_of_element_located(locator))</span><br><span class="line"></span><br><span class="line"># 通过xpath的方式查找</span><br><span class="line">su = driver.find_element_by_xpath(&#x27;//*[@id=&quot;su&quot;]&#x27;)</span><br><span class="line"></span><br><span class="line"># print(su.get_attribute(&#x27;type&#x27;))</span><br><span class="line"># print(su.get_attribute(&#x27;id&#x27;))</span><br><span class="line"># print(su.get_attribute(&#x27;value&#x27;))</span><br><span class="line"># print(su.get_attribute(&#x27;class&#x27;))</span><br><span class="line"></span><br><span class="line"># # 通过标签的id查找</span><br><span class="line"># su = driver.find_element_by_id(&#x27;su&#x27;)</span><br><span class="line"># # 通过标签的css选择器查找</span><br><span class="line"># su = driver.find_element_by_css_selector(&#x27;#su&#x27;)</span><br><span class="line"># # 通过class进行查找</span><br><span class="line"># driver.find_element_by_class_name(&#x27;bg s_btn&#x27;)</span><br><span class="line"></span><br><span class="line"># 也是通过标签的xpath，等同于 driver.find_element_by_xpath(&#x27;//*[@id=&quot;su&quot;]&#x27;)</span><br><span class="line"># driver.find_element(By.XPATH, &#x27;//*[@id=&quot;su&quot;]&#x27;)</span><br><span class="line"></span><br><span class="line"># print(driver.title)</span><br><span class="line"># print(su.get_attribute(&#x27;value&#x27;))</span><br><span class="line"></span><br><span class="line"># 保存屏幕</span><br><span class="line">driver.get_screenshot_as_file(&#x27;screenshot.jpg&#x27;)</span><br><span class="line"></span><br><span class="line"># 需要手动退出driver</span><br><span class="line"># 切记切记一定退出</span><br><span class="line">driver.quit()</span><br><span class="line"></span><br><span class="line">3.2</span><br><span class="line"># sina移动端 ，火狐</span><br><span class="line">from selenium import webdriver</span><br><span class="line">from selenium.webdriver.support.wait import WebDriverWait</span><br><span class="line">from selenium.webdriver.support import expected_conditions as EC</span><br><span class="line">from selenium.webdriver.common.by import By</span><br><span class="line">import time</span><br><span class="line"></span><br><span class="line">#打开浏览器</span><br><span class="line">driver = webdriver.Firefox()</span><br><span class="line"># 设置10秒页面超时返回，类似于requests.get()的timeout选项，driver.get()没有timeout选项</span><br><span class="line"># 以前遇到过driver.get(url)一直不返回，但也不报错的问题，这时程序会卡住，设置超时选项能解决这个问题。</span><br><span class="line">driver.set_page_load_timeout(10)</span><br><span class="line"># 设置10秒脚本超时时间</span><br><span class="line">driver.set_script_timeout(10)</span><br><span class="line">driver.set_window_size(1366, 768)</span><br><span class="line"></span><br><span class="line"># 访问新浪移动端，没有验证码</span><br><span class="line">driver.get(&#x27;https://passport.weibo.cn/signin/login?entry=mweibo&amp;res=wel&amp;wm=3349&amp;r=http%3A%2F%2Fm.weibo.cn%2F&#x27;)</span><br><span class="line"></span><br><span class="line">WebDriverWait(driver, 30, 1).until(EC.presence_of_element_located((By.XPATH, &#x27;//*[@id=&quot;loginName&quot;]&#x27;)))</span><br><span class="line"></span><br><span class="line">print(driver.title)</span><br><span class="line"></span><br><span class="line">time.sleep(1)</span><br><span class="line"></span><br><span class="line">user = driver.find_element_by_xpath(&#x27;//*[@id=&quot;loginName&quot;]&#x27;)</span><br><span class="line"># 清除当前input元素中的值，需要清除</span><br><span class="line">user.clear()</span><br><span class="line"># 在input元素中输入内容</span><br><span class="line">user.send_keys(&#x27;51508690@qq.com&#x27;)</span><br><span class="line"></span><br><span class="line">pwd = driver.find_element_by_xpath(&#x27;//*[@id=&quot;loginPassword&quot;]&#x27;)</span><br><span class="line">pwd.clear()</span><br><span class="line">pwd.send_keys(&#x27;mumu2018&#x27;)</span><br><span class="line"></span><br><span class="line">login = driver.find_element_by_xpath(&#x27;//*[@id=&quot;loginAction&quot;]&#x27;)</span><br><span class="line"># 出发这个login元素的click事件</span><br><span class="line">login.click()</span><br><span class="line"></span><br><span class="line">WebDriverWait(driver, 30, 1).until(EC.presence_of_element_located((By.XPATH, &#x27;//p[@data-node=&quot;title&quot;]&#x27;)))</span><br><span class="line"></span><br><span class="line">msg = driver.find_element_by_xpath(&#x27;/html/body/div[1]/div[1]/div[1]/div[2]/a[2]&#x27;)</span><br><span class="line">msg.click()</span><br><span class="line"></span><br><span class="line"># 需要手动退出driver</span><br><span class="line">driver.quit()</span><br><span class="line"></span><br><span class="line">print(&#x27;over&#x27;)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2><span id="四-注意事项">四、注意事项</span></h2><blockquote>
<p>1、需要手动关闭<br>driver.quit()</p>
<p>2、并发使用多进程</p>
<p>3、html中有iframe<br>需要切换到iframe中去：driver.switch_to.frame(driver.find_element_by_id(“topmenuFrame”))<br>然后查找iframe下的元素<br>切换回默认的frame中：driver.switch_to.frame(0)</p>
<p>4、三种等待资源加载完成<br>4.1、 time.sleep<br>强制等待，不管资源的实际加载情况，等待指定时长，时间到后直接执行后续代码</p>
<p>4.2、 driver.implicitly_wait(30)<br>隐性等待，设置一个最长等待时间，如果资源提前加载，则会立即往下执行<br>注意这个设置为全局设置，对整个driver的所有资源加载都有效，只需要在开始设置一次<br>弊端：会等待所有资源加载完成</p>
<p>4.3、 显性等待<br>from selenium.webdriver.support.wait import WebDriverWait<br>from selenium.webdriver.support import expected_conditions as EC<br>driver.get(‘<a target="_blank" rel="noopener" href="https://huilansame.github.io'/">https://huilansame.github.io'</a>)<br>locator &#x3D; (By.LINK_TEXT, ‘CSDN’)<br>WebDriverWait(driver, 20, 0.5).until(EC.presence_of_element_located(locator))</p>
<p>5、execute_script ： 执行任意的js脚本</p>
</blockquote>
<h2><span id="五-phantomjs无界面浏览器">五、phantomjs无界面浏览器</span></h2><blockquote>
<p>官方文档：<a target="_blank" rel="noopener" href="http://phantomjs.org/api/webpage/">http://phantomjs.org/api/webpage/</a></p>
</blockquote>
<h2><span id="六-下载并安装对应的浏览器">六、下载并安装对应的浏览器</span></h2><ul>
<li>自行下载安装chrome</li>
<li>在这个页面找对应的chromedriver下载：<a target="_blank" rel="noopener" href="http://chromedriver.storage.googleapis.com/index.html">http://chromedriver.storage.googleapis.com/index.html</a></li>
</ul>
<h1><span id="八-cookie的应用">八、cookie的应用</span></h1><h2><span id="一-cookie是什么">一、cookie是什么？</span></h2><blockquote>
<p>Cookie，有时也用其复数形式 Cookies，指某些网站为了辨别用户身份、<br>进行 session 跟踪而储存在用户本地终端上的数据（通常经过加密）。<br>定义于 RFC2109 和 2965 中的都已废弃，最新取代的规范是 RFC6265[1]。<br>Cookie其实就是浏览器缓存</p>
</blockquote>
<h2><span id="二-cookie的生命周期">二、cookie的生命周期</span></h2><blockquote>
<p>1、会话cookie：没有设置expires（是个时间戳）的，浏览器（session）关闭后，就自动失效<br>2、持久cookie：设置了expires的，根据设置的失效时间决定（expires时间是可以进行修改的，<br>但是很多网站会做防止修改的设置）</p>
</blockquote>
<h2><span id="三-cookie具有的属性">三、cookie具有的属性</span></h2><blockquote>
<p>name：为一个cookie的名称。<br>value：为一个cookie的值。</p>
<p>domain：为可以访问此cookie的域名，譬如<a href="http://www.baidu.com:baidu.xn--com-t33er8ouvhvrt1f0c6s9b/">www.baidu.com：baidu.com就是顶级域名</a><br>1、非顶级域名，如二级域名或者三级域名，设置的cookie的domain只能为顶级域名或者二级域名或者三级域名本身，不<br>能设置其他二级域名的cookie，否则cookie无法生成。<br>passport.baidu.com 这个域名下，你只能设置 domain 为 passport.baidu.com 或 baidu.com ， 不能设置 passport1.baidu.com<br>2、顶级域名只能设置domain为顶级域名，不能设置为二级域名或者三级域名，否则cookie无法生成。<br>baidu.com 只能设置 domain 为 baidu.com<br>3、二级域名能读取设置了domain为顶级域名或者自身的cookie，不能读取其他二级域名domain的cookie。<br>所以要想cookie在多个二级域名中共享，需要设置domain为顶级域名，<br>这样就可以在所有二级域名里面或者到这个cookie的值了。<br>4、顶级域名只能获取到domain设置为顶级域名的cookie，其他domain设置为二级域名的无法获取。</p>
<p>path：为可以访问此cookie的页面路径。 比如<a target="_blank" rel="noopener" href="http://www.baidu.com/path%EF%BC%8Cpath%E5%B0%B1%E6%98%AF/test%EF%BC%8C">www.baidu.com/path，path就是/test，</a><br>path &#x3D; &#x2F;test 设置为： <a target="_blank" rel="noopener" href="http://www.baidu.com/test%EF%BC%8C">www.baidu.com/test，</a> 那么只有&#x2F;test路径下的页面可以读取此cookie。<br>如果想所有路径都能访问，就设置 path &#x3D; &#x2F;</p>
<p>expires&#x2F;Max-Age ：为此cookie超时时间。若设置其值为一个时间(一个时间戳)，那么当到达此时间后，此cookie失效。<br>不设置的话默认值是Session，意思是cookie会和session一起失效。<br>当浏览器关闭(不是浏览器标签页，而是整个浏览器) 后，此cookie失效。</p>
<p>Size： 此cookie大小。<br>http： cookie的httponly属性。若此属性为true，则只有在http请求头中会带有此cookie的信息，<br>而不能通过document.cookie来访问此cookie。<br>secure ： boolean型，默认为false，设置是否只能通过https来传递此cookie</p>
<p>爬虫重点关注name和value</p>
</blockquote>
<h2><span id="四-cookie应用">四、cookie应用</span></h2><h3><span id="1-基础应用">1、基础应用</span></h3><blockquote>
<ul>
<li><p>1、以任何方式，如浏览器、selenium、封包方式等，获得对应的cookies</p>
</li>
<li><p>2、将cookies保存，可以是在内存、文件、数据库等</p>
</li>
<li><p>3、在你想要应用已有的cookie的 项目 中，已各种方式：文件、数据库、网络等，获取到对应的cookie，然后进行设置到请求的headers中的cookie中，接着就可以访问对应的资源了。</p>
<h5><span id="实际应用">实际应用：</span></h5></li>
<li><p>1、有几台专门的服务器，进行登录操作，所有账号保存在数据库，</p>
</li>
</ul>
<p>由这些专门登录的服务器进行登录操作，登录成功后，保存cookies到数据库</p>
<ul>
<li>2、有专门的应用服务器，从数据库读取cookies，进行相应的业务操作这种服务器不处理登录操作</li>
</ul>
<h5><span id="ps">PS：</span></h5><ul>
<li>1、cookie的应用，必须是服务器支持，不同的session可以使用同一个cookie</li>
<li>2、expires是一个客户端和服务器的君子约定，浏览器检测到失效了，就不会读取这个cookie，大部分网站都不会检测这个cookie失效，部分要求严格的网站是会检测的，和服务器时间进行比对，判断是否失效。一般都通过md5设置</li>
<li>3、cookie都是由服务器设置的，你客户端设置没意义，服务器不会进行验证</li>
</ul>
<p>通过response的headers中set-cookie 设置</p>
</blockquote>
<h3><span id="2-selenium只会获得当前域名的cookie">2、selenium只会获得当前域名的cookie</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">复制# selenium 设置cookie，只会得到当前域名，如果要获取多个子域名下的cookie，需要分别进行访问获取，并且进行合并</span><br><span class="line">例如：</span><br><span class="line">driver.get(&#x27;http://www.baidu.com&#x27;)</span><br><span class="line">cookies = driver.get_cookies()</span><br><span class="line"></span><br><span class="line"># 把需要获取cookie的域名进行访问，并且将多个域名的cookie进行合并保存</span><br><span class="line">driver.get(&#x27;http://passport.baidu.com&#x27;)</span><br><span class="line">cookies.extend(driver.get_cookies())</span><br><span class="line"></span><br><span class="line"># 应用时，最简单的就是讲 domain 设置为 顶级域名：</span><br><span class="line">file = &#x27;baidu_cookies_update.txt&#x27;</span><br><span class="line">with open(file, &#x27;r&#x27;) as f:</span><br><span class="line">    cookies = json.load(f)</span><br><span class="line">    for cookie in cookies:</span><br><span class="line">        cookie[&#x27;domain&#x27;] = &#x27;.baidu.com&#x27;</span><br><span class="line">        driver.add_cookie(cookie)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h1><span id="九-pyexecjs脚本执行语言">九、PyExecJS脚本执行语言</span></h1><h2><span id="一-介绍">一、介绍</span></h2><blockquote>
<ul>
<li><p>PyExecJS是Ruby的ExecJS移植</p>
</li>
<li><p>PyExecJS 用于在python中执行js脚本</p>
</li>
<li><p>官网：<a target="_blank" rel="noopener" href="https://pypi.python.org/pypi/PyExecJS">https://pypi.python.org/pypi/PyExecJS</a></p>
</li>
<li><p>PyExecJS其实是调用的windows系统的 CScript.exe 来执行的JS代码</p>
<p>这个exe就是用来执行脚本，一般脚本是vbscript和javascript</p>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">这个exe可以直接在cmd中通过 cscript.exe name.js</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<ul>
<li><p>不管任何的第三方js执行库，最终要执行js代码，必须要有一个js引擎</p>
</li>
<li><p>碰到自定义的加密算法，只有2种途径处理（不考虑selenium）：</p>
<p>1、就是用python实现，</p>
<p> 2、js调用，这个是有局限的，只能调用纯算法的函数，不能牵扯到document的上下文</p>
</li>
<li><p>selenium属于浏览器模拟，不需要理会js等东西的处理，</p>
<p>但是碰上有行为分析的网站，就很可能被察觉为非人类</p>
</li>
<li><p>pyexecjs，在调用纯加密算法的js代码，可以提供很大的便利，直接调用执行，不需要分析算法</p>
</li>
<li><p>动态内容怎么处理，就是页面加载完成后，js通过ajax方式获取的内容</p>
</li>
</ul>
<p>答：对于封包爬虫来说，是否是ajax的，或者是url点击，或者是post提交等等，<br>都是没有区别的。<br>都是通过抓包，获取到请求（ajax的请求是post），只要抓到包，<br>进行模拟，获取所有提交的参数，直接获取或者加密得到<br>注意只要别漏包就行了。<br>对于封包爬虫来说，<br>1、能够抓取到所有的交互请求<br>2、能够得到所有的交互提交的参数（1是直接获取，2是加密得来）<br>3、不需要理会这个交互的触发方式</p>
</blockquote>
<h2><span id="二-实例代码">二、实例代码</span></h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">复制import execjs</span><br><span class="line"></span><br><span class="line"># 最基本的使用，执行js代码</span><br><span class="line"># print(execjs.eval(&quot;&#x27;red yellow blue&#x27;.split(&#x27; &#x27;)&quot;))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 获取默认的execjs对象，然后通过eval执行js代码</span><br><span class="line"># ctx = execjs.get()</span><br><span class="line"># a = ctx.eval(&quot;1 + 2&quot;)</span><br><span class="line"># print(a)</span><br><span class="line"></span><br><span class="line"># 执行js代码中指定的函数</span><br><span class="line"># ctx = execjs.compile(&quot;&quot;&quot;</span><br><span class="line"># function add(x, y) &#123;</span><br><span class="line">#     return x + y;</span><br><span class="line"># &#125;</span><br><span class="line"># &quot;&quot;&quot;)</span><br><span class="line"># b = ctx.call(&#x27;add&#x27;, 1, 2)</span><br><span class="line"># print(b)</span><br><span class="line"></span><br><span class="line">import execjs</span><br><span class="line"></span><br><span class="line">js = &#x27;&#x27;&#x27;</span><br><span class="line">function createGuid() &#123;</span><br><span class="line">	return (((1 + Math.random()) * 0x10000) | 0).toString(16).substring(1);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">function get()&#123;</span><br><span class="line">    var guid = createGuid() + createGuid() + &quot;-&quot; + createGuid() + &quot;-&quot; + createGuid() + createGuid() + &quot;-&quot; + createGuid() + createGuid() + createGuid(); //CreateGuid();</span><br><span class="line">    return guid;</span><br><span class="line">&#125;</span><br><span class="line">&#x27;&#x27;&#x27;</span><br><span class="line">ctx = execjs.compile(js)</span><br><span class="line">b = ctx.call(&#x27;get&#x27;)</span><br><span class="line">print(b)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h1><span id="十-aiohttp">十、aiohttp</span></h1><h2><span id="一-介绍">一、介绍</span></h2><blockquote>
<p>aiohttp 是一个异步 http 通讯的 服务端和客户端 框架</p>
<p>项目地址：<a target="_blank" rel="noopener" href="https://github.com/aio-libs/aiohttp">https://github.com/aio-libs/aiohttp</a><br>官方网站：<a target="_blank" rel="noopener" href="https://pypi.python.org/pypi/aiohttp">https://pypi.python.org/pypi/aiohttp</a><br>官方文档：<a target="_blank" rel="noopener" href="https://aiohttp.readthedocs.io/en/stable/">https://aiohttp.readthedocs.io/en/stable/</a><br>官方教程：<a target="_blank" rel="noopener" href="https://aiohttp.readthedocs.io/en/stable/tutorial.html#aiohttp-tutorial">https://aiohttp.readthedocs.io/en/stable/tutorial.html#aiohttp-tutorial</a></p>
</blockquote>
<h2><span id="二-安装">二、安装</span></h2><blockquote>
<p>基于3.1.0<br>1、使用 pip install aiohttp<br>2、同时可以安装2个配套的 异步库，<br>pip install cchardet：文本识别的库<br>pip install aiodns： DNS 解析的库</p>
<p>3、还有一些其他异步库，都不是很成熟，如文件操作的</p>
</blockquote>
<h2><span id="三-使用">三、使用</span></h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line">复制# 1、async ：</span><br><span class="line">协程 函数定义，都是  async def fun_name  这样定义。</span><br><span class="line"></span><br><span class="line"># 2、async with：</span><br><span class="line">协程 上下文管理器 函数 调用 async with aiohttp.ClientSession() as client</span><br><span class="line"></span><br><span class="line"># 3、await：</span><br><span class="line">在 协程函数内容，即  async def 定义的函数内部，调用所有 协程函数都是使用</span><br><span class="line">    await async_fun()</span><br><span class="line">    text = await response.text()</span><br><span class="line"></span><br><span class="line"># 4、loop</span><br><span class="line">loop = asyncio.get_event_loop()</span><br><span class="line">loop.run_until_complete(asyncio.gather(*tasks))</span><br><span class="line">loop.close() # 项目中多个loop时，不清楚的情况下，宁愿不调用这个close</span><br><span class="line"></span><br><span class="line"># 5、aiohttp.ClientSession</span><br><span class="line">参数(重点)：</span><br><span class="line">cookies :  用于传入指定的cookie，字典</span><br><span class="line">headers：  用于指定所有请求的默认headers</span><br><span class="line">trust_env: 是否获取系统的代理设置</span><br><span class="line"></span><br><span class="line">方法：</span><br><span class="line">get、post、head等</span><br><span class="line">方法的参数：</span><br><span class="line">ssl = None</span><br><span class="line">proxy = &#123;&#125;</span><br><span class="line">allow_redirects = True</span><br><span class="line">timeout：超时时间，单位秒。aiohttp的所有io操作，默认都是5分钟的超时时间</span><br><span class="line"></span><br><span class="line">参数提交：</span><br><span class="line">params</span><br><span class="line">data</span><br><span class="line">json</span><br><span class="line"></span><br><span class="line"># 6、ClientResponse</span><br><span class="line">属性：</span><br><span class="line">status：200</span><br><span class="line">reason：ok</span><br><span class="line">method：get</span><br><span class="line">url：页面url</span><br><span class="line">cookies：http.cookies.SimpleCookie</span><br><span class="line">charset：页面的charset</span><br><span class="line">content_type：页面的content_type</span><br><span class="line"></span><br><span class="line">方法（记住，都是方法，调用的后面有小括号）：</span><br><span class="line">read()：返回bytes ， 等同于 requests中 content</span><br><span class="line">text(encoding=None)：页面的text， 可以带入编码</span><br><span class="line">json(encoding=None)：页面的内容json</span><br><span class="line"></span><br><span class="line"># 7、asyncio.Semaphore</span><br><span class="line">用于控制并发的数量，不是线程安全的。</span><br><span class="line">定义：sem = Semaphore(10)</span><br><span class="line">调用：with await sem:</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2><span id="四-注意事项">四、注意事项：</span></h2><ul>
<li>async 函数中 sleep 只能使用 asyncio.sleep()</li>
<li>loop.close() ，如果是有多个 loop， 要特别注意，别互相弄混</li>
</ul>
<h2><span id="五-aio原理">五、aio原理</span></h2><blockquote>
<p>1、改写原本的阻塞的io 方法<br>2、改成内核通知的方式，从之前等待，改成类似回调函数，执行io命令之后，挂起本函数，并且执行下一个 task<br>3、当io事件完成或者异常时，通过系统通知之前注册事件并且挂起的函数，继续执行</p>
</blockquote>
<h2><span id="六-实例">六、实例</span></h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">复制# 简单请求示例</span><br><span class="line">import aiohttp</span><br><span class="line">import asyncio</span><br><span class="line">import async_timeout</span><br><span class="line"></span><br><span class="line">async def fetch(session, url):</span><br><span class="line">    async with async_timeout.timeout(10):</span><br><span class="line">        async with session.get(url) as response:</span><br><span class="line">            return await response.text()</span><br><span class="line"></span><br><span class="line">async def main():</span><br><span class="line">    async with aiohttp.ClientSession() as session:</span><br><span class="line">        html = await fetch(session, &#x27;http://www.baidu.com&#x27;)</span><br><span class="line">        print(html)</span><br><span class="line"></span><br><span class="line">loop = asyncio.get_event_loop()</span><br><span class="line">loop.run_until_complete(main())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># sem 示例</span><br><span class="line">import random</span><br><span class="line">import asyncio</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">async def compute(x, y):</span><br><span class="line">    print(&quot;Compute %s + %s ...&quot; % (x, y))</span><br><span class="line">    await asyncio.sleep(random.random())</span><br><span class="line">    return x + y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">async def print_sum(x, y, sem):</span><br><span class="line">    # 这里控制并发的任务数</span><br><span class="line">    async with sem:</span><br><span class="line">        result = await compute(x, y)</span><br><span class="line">        print(&quot;%s + %s = %s&quot; % (x, y, result))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">loop = asyncio.get_event_loop()</span><br><span class="line"># 控制并发数</span><br><span class="line">sem = asyncio.Semaphore(5)</span><br><span class="line">loop.run_until_complete(asyncio.gather(*[print_sum(i, i + 1, sem) for i in range(100)]))</span><br><span class="line">loop.close()</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h1><span id="十一-验证码">十一、验证码</span></h1><h2><span id="一-什么是验证码">一、什么是验证码？</span></h2><blockquote>
<p><strong>验证码（CAPTCHA）是“Completely Automated Public Turing test to tell Computers and Humans Apart”（全自动区分计算机和人类的图灵测试）的缩写，是一种区分用户是计算机还是人的公共全自动程序。可以防止：恶意破解密码、刷票、论坛灌水，有效防止某个黑客对某一个特定注册用户用特定程序暴力破解方式进行不断的登陆尝试。这个问题可以由计算机生成并评判，但是必须只有人类才能解答。由于计算机无法解答CAPTCHA的问题，所以回答出问题的用户就可以被认为是人类。</strong></p>
</blockquote>
<h2><span id="二-验证码类别">二、验证码类别</span></h2><blockquote>
<p><del>一个合格的验证码，是能够隔离大部分的机器人，而不让普通用户感觉到厌烦，客户体验很好。</del></p>
<ul>
<li>1、传统输入式验证码:</li>
</ul>
<p>由多个 数字、符号、字母、汉字 组成，加一些干扰项，如点状、线状干扰图案，扭曲本身内容<br>还有那种动态的晃来晃去的。<br>特点：部署简单，技术也最简单<br>缺点：防护性不高<br>突破技术：<br>1、本机自动识别图片，识别率不是很高，代表性的，<br>如：sunday.dll（针对不同的网站进行调优，识别率达到50%-60%）<br>pytesseract-oct：机器学习的进行识别<br>2、可以架设服务器做机器学习的方式进行训练，识别验证码<br>如：各大主流的验证码打码平台，都有这样的实现<br>3、通过打码平台进行打码，发送图片到打码平台，平台返回图片内容。现在最主流的<br>4、通过第三方免费的图片识别技术进行验证码识别，如百度识图，google识图。</p>
<ul>
<li>2、简单问题型验证码：</li>
</ul>
<p>如回答本网站域名，简单的1+3&#x3D;？ 等这样的问题<br>特点：部署比较简单，技术也简单<br>缺点：防护性一般，但是用户体验稍差<br>突破技术：<br>1、纯计算的题，或者那种简单选择出现的字符的题，是可以通过图片识别进行验证通过的<br>2、部分网站，是可以把所有题目穷尽，全部下载到本地，把图片做相应处理，<br>然后和需要验证的时候的验证码进行匹配，进行识别<br>3、打码平台</p>
<ul>
<li>3、纯行为验证码：</li>
</ul>
<p>采用极验的：<a target="_blank" rel="noopener" href="https://account.geetest.com/register%EF%BC%8C%E8%99%8E%E5%97%85">https://account.geetest.com/register，虎嗅</a><br>拖动图片或点击元素后，拖动图片，如：极验验证码<br>无法使用打码平台，因为这个行为验证的原理，就是本地js捕捉你的鼠标事件<br>如开始移动事件，单位时间移动距离，上下相对位置的偏移，鼠标的运动轨迹，<br>前后鼠标移动速度的对比，最终鼠标停止移动的坐标等。。。。<br>特点：部署麻烦，防护性强<br>缺点：需要好体验的话需要费用<br>突破技术：<br>1、浏览器模拟(难度较大)，譬如：selenium+phantomjs，<br>要模拟正常用户的拖动，不能瞬移，不能完全直线，速度不能匀速，得上下轻微移动<br>2、js破解(难度很大)：还原这个完整图片，查找这个缺口坐标，破解js函数，获取必要的计算参数，<br>去进行模拟js的计算，得出一系列的值，模拟提交</p>
<ul>
<li>4、图标选择</li>
</ul>
<p>给出一组图片，按要求点击其中一张或者多张，如：12306、新浪注册（九宫格）<br>特点：部署稍难，防护性一般<br>缺点：客户体验一般<br>突破技术：<br>1、图片识别<br>2、打码平台，一般需要订制打码服务</p>
<ul>
<li>5、点击式的图文验证与行为辅助</li>
</ul>
<p>通过文字提醒用户点击图中相同字的位置进行验证。<br>特点：部署稍难<br>缺点：客户体验差<br>突破技术：<br>1、图片识别后，点击目标坐标<br>2、打码平台，订制服务</p>
<ul>
<li>6、手机获取验证码</li>
</ul>
<p>通过输入指定手机号码，获取验证码，进行验证。<br>解决方案：手机打码平台</p>
<ul>
<li>7、手机发送短信</li>
</ul>
<p>通过手机发送特定短信到特定号码，进行验证。<br>解决方案：手机打码平台</p>
</blockquote>
<h2><span id="三-处理方式">三、处理方式</span></h2><blockquote>
<ul>
<li>1、使用百度识图、google识图等第三方API识别图片</li>
</ul>
<p>曾经 12306可以使用百度识图进行识别<br>曾经 <a target="_blank" rel="noopener" href="http://cn.docs88.com/%E4%B9%9F%E5%8F%AF%E4%BB%A5%E8%AF%86%E5%88%AB%E7%AE%80%E5%8D%95%E7%9A%84%E5%9B%BE%E7%89%87">http://cn.docs88.com/也可以识别简单的图片</a></p>
<ul>
<li>2、通过人工智能识别图片</li>
</ul>
<p>大部分打码平台都是AI+打码员人工打码结合</p>
<ul>
<li><p>3、pytesseract识别图片</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">复制&gt;   # 在使用pytesseract前，需要下载安装tesseract</span><br><span class="line">&gt;   # 打开pytesseract.py文件,</span><br><span class="line">&gt;   # 修改参数 tesseract_cmd = r&#x27;D:\tesseract\Tesseract-OCR\tesseract.exe&#x27;</span><br><span class="line">&gt;   # 在安装目录的Tesseract-OCR\tessdata文件夹中添加中文语言包</span><br><span class="line">&gt;</span><br><span class="line">&gt;   from PIL import Image</span><br><span class="line">&gt;   import pytesseract</span><br><span class="line">&gt;</span><br><span class="line">&gt;   text=pytesseract.image_to_string(Image.open(&#x27;1.png&#x27;), lang=&#x27;chi_sim&#x27;)</span><br><span class="line">&gt;   print(text)</span><br><span class="line">&gt;</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
</ul>
</blockquote>
<blockquote>
<p>chi_sim 是中文字符集 eng 是英文字符集</p>
<p> 1.1 安装PILLOW，在pycharm或者pip安装<br>1.2 安装pytesseract，在pycharm或者pip安装<br>1.3 tesseract安装指南：<a target="_blank" rel="noopener" href="https://jingyan.baidu.com/article/219f4bf788addfde442d38fe.html">https://jingyan.baidu.com/article/219f4bf788addfde442d38fe.html</a><br>1.4 <a target="_blank" rel="noopener" href="https://github.com/tesseract-ocr/tessdata">https://github.com/tesseract-ocr/tessdata</a> 下载训练内容<br><a target="_blank" rel="noopener" href="https://github.com/tesseract-ocr/langdata">https://github.com/tesseract-ocr/langdata</a> 下载语言数据<br>1.5 文档：<a target="_blank" rel="noopener" href="https://github.com/tesseract-ocr/tesseract/wiki/ImproveQuality">https://github.com/tesseract-ocr/tesseract/wiki/ImproveQuality</a><br><a target="_blank" rel="noopener" href="https://github.com/tesseract-ocr/tesseract/blob/master/doc/tesseract.1.asc">https://github.com/tesseract-ocr/tesseract/blob/master/doc/tesseract.1.asc</a><br>1.6 需要VC2015</p>
<ul>
<li>4、通过打码平台识别验证码</li>
</ul>
<p> 4.1 图片打码平台识别图片<br>4.2 手机打码平台完成手机验证码</p>
</blockquote>
<h1><span id="十二-爬虫代理ip的处理">十二、爬虫代理ip的处理</span></h1><h2><span id="一-什么叫做反封ip处理">一、什么叫做反封IP处理</span></h2><blockquote>
<p>爬虫进行爬取的时候，目标服务器有可能对单位时间内爬取频率超过一定数值的IP进行禁止访问的处理，<br>绕过这种服务器防护手段，稳定的进行爬取，这种绕过的手段就叫反封IP处理</p>
</blockquote>
<h2><span id="二-如何反封ip">二、如何反封IP</span></h2><h3><span id="1-降低频率">1、降低频率</span></h3><blockquote>
<p>部分网站虽然有封IP的处理，但是可以降低爬取的频率，测试服务器的检测阀值，<br>如果在不触及服务器阀值的情况下，爬取速度能够满足我们的应用需求，那么就直接这么处理就好。<br>优点：简单，稳定<br>缺点：速度慢，效率低<br>原理：测试服务器的拒绝服务的阀值(server_test测试)<br>有部分网站是没有做任何IP限制的，这种网站建议大家不要太狠，一般还是限制一个单位的访问次数，不然服务器会宕机<br>得到阀值后，不让服务器检测到你异常，不断的增加延时的时间，判断单位时间内的最大访问次数<br>譬如，你测算出服务器的允许时间是：1秒1次请求，然后你的爬虫任务是5秒完成一次，<br>设置5个线程（协程）进行网络访问，每个任务计算消耗时间，如果低于5秒了，就延时</p>
<p>这种方式对于数据价值比较加高的应用，是可取的，可以增服务器来提高单位时间内的爬取数据</p>
</blockquote>
<h3><span id="2-本机宽带拨号或使用拨号vps">2、本机宽带拨号或使用拨号VPS</span></h3><blockquote>
<p>每次拨号后，会获得一个新的IP地址<br>通过代码或者第三方拨号软件定时拨号，更换机器IP<br>优点：较简单，成本低<br>缺点：拨号有时IP重复，需要多次拨号，效率较低<br>原理就是：<br>不管服务器的拒绝服务的阈值，遇到被限制了，就进行拨号，更换IP<br>一个宽带账号，是在一个固定的IP段内的，重复拨号，有可能连续拨到同一个IP（处理方式可以建立一个已使用的IP列表，拨号后进行检测）<br>严重的时候，会被服务器把IP段给封掉<br>一般来讲也不会太多的线程处理，过快拨号，会把系统弄宕机</p>
<p>连接了路由的机器，可以通过路由器的接口进行拨号处理</p>
</blockquote>
<h3><span id="3-代理ip最常用的方式">3、代理IP，最常用的方式</span></h3><blockquote>
<p>使用代理IP，不管是付费的还是免费的，都经常有获取的ip不能使用的情况<br>3.1、免费代理：使用程序从一些网站上搜索到免费代理IP，进行测试后，维护起来，形成一个IP池，不断地更新<br>优点：复杂，成本低<br>缺点：不稳定，效率稍低<br>原理：<br>就是在我们的requests包中，<br>session &#x3D; requests.session()<br>session.proxy &#x3D; 设置这个代理的ip和端口<br>代理IP的数据流（其实就是一个数据转发）：<br>发送： 本机发送一个http请求》》请求发送到代理IP的port，代理服务器做了处理后》》代理服务器发送到目标服务器<br>返回：目标服务器返回应答内容》》应答内容发送到代理服务器》》发送到本机</p>
<p>经过代理后的数据请求，在目标服务器（譬如百度）看来，访问它的机器的IP是代理服务器的IP地址，而不是你的本机IP</p>
<p>爬虫不使用代理软件，直接通过设置proxy进行代理ip的访问</p>
<p>代理IP转发你的http请求，是因为它机器上有代理服务的软件，会解析请求（只支持部分协议数据的解析），获得目标服务器，然后把数据发送过去</p>
<p>3.2维护代理池</p>
<p>我们可以自己维护一个代理池：<br>1、通过爬虫，爬取 免费代理网站 上爬取免费的 代理IP和端口，<br>2、爬到的数据可以保存在内存、文件、数据库，保存的过程中需要对爬取到的IP和PORT进行去重<br>3、对新获取的IP和PORT，进行检测，检测的方式，根据你的当前的系统功能，设置proxy，连接你的业务的目标服务器，看是否可以正常通信，检测可以根据不同的业务系统进行不同的检测，OK的保存到一个可以使用的IP列表中，不OK的，丢弃或者保存到另外一个异常IP列表中</p>
<p>维护的ip池信息<br>ip port 获取时间 端口是否可用 projectA是否可用 projectB是否可用 是否已经检测过</p>
<p>4、业务系统运行时，从IP池中获取本系统可用的IP和PORT，设置为proxy，进行业务访问<br>5、当前IP和PORT异常后，丢弃或标志不可使，并重新获取可用的IP和端口</p>
<p>示例：<br>1台机器不断爬取代理IP和PORT信息，保存到数据库中，通过数据库进行去重，<br>可以使用获取代理的机器进行IP和PORT的检测，也可以单独开一台机器进行检测，<br>多台机器运行业务系统，不断从数据库中提取可用的代理IP和PORT</p>
<p>3.2、付费代理：通过网上一些代理IP服务商购买代理IP服务，使用提供的接口获取代理列表就行<br>优点：简单，稳定，效率高<br>缺点：成本高<br>原理：<br>代理提供商维护一批可用的IP地址，在你提取IP时，发送给你，稳定性根据不同的服务商不同</p>
</blockquote>
<h3><span id="4-总结">4、总结</span></h3><blockquote>
<p><strong>1、手动操作，次数多了一样会被封IP，一般封多长时间，譬如半小时，服务器也怕是误检测</strong><br><strong>2、反封IP就2种：1、不被检测出来，2、被检测了，更换IP</strong></p>
</blockquote>
<h1><span id="十三-scrapy框架学习">十三、scrapy框架学习</span></h1><h2><span id="一-安装">一、安装</span></h2><blockquote>
<p><strong>pip install scrapy</strong></p>
</blockquote>
<h2><span id="二-创建项目">二、创建项目</span></h2><blockquote>
<p>E:\爬虫&gt;scrapy startproject scrapy_Test</p>
</blockquote>
<h2><span id="三-详解">三、详解</span></h2><blockquote>
<p>项目地址：<a target="_blank" rel="noopener" href="https://github.com/scrapy/scrapy">https://github.com/scrapy/scrapy</a></p>
<p>官方网站：<a target="_blank" rel="noopener" href="https://scrapy.org/">https://scrapy.org/</a></p>
<p>官方文档：<a target="_blank" rel="noopener" href="https://docs.scrapy.org/en/latest/">https://docs.scrapy.org/en/latest/</a></p>
<p>官方教程：<a target="_blank" rel="noopener" href="https://docs.scrapy.org/en/latest/intro/tutorial.html">https://docs.scrapy.org/en/latest/intro/tutorial.html</a></p>
</blockquote>
<h3><span id="1-settings文件配置">1、settings文件配置</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">复制# 所有的key 都是全部大写！</span><br><span class="line"></span><br><span class="line">BOT_NAME = &#x27;firstscrapy&#x27; # 项目名</span><br><span class="line">一、ROBOTSTXT_OBEY = Ture，是否遵守 robots.txt， 修改为False</span><br><span class="line">二、DEFAULT_REQUEST_HEADERS ： 默认的请求headers,需要设置</span><br><span class="line">三、# 开发模式时，把下面的注释取消掉，启用缓存，可以提高调试效率</span><br><span class="line"># 同样的请求，如果 缓存 当中有保存内容的话，不会去进行网络请求，直接从缓存中返回</span><br><span class="line"># 记住：开发环境下启用！！！！部署时一定要注释掉！！！</span><br><span class="line">HTTPCACHE_ENABLED = True</span><br><span class="line">HTTPCACHE_EXPIRATION_SECS = 0</span><br><span class="line">HTTPCACHE_DIR = &#x27;httpcache&#x27;</span><br><span class="line">HTTPCACHE_IGNORE_HTTP_CODES = []</span><br><span class="line">HTTPCACHE_STORAGE = &#x27;scrapy.extensions.httpcache.FilesystemCacheStorage&#x27;</span><br><span class="line">------------------------------------------------------------------------------------------------</span><br><span class="line">PROXIES = [</span><br><span class="line">    &#123;&#x27;ip_port&#x27;: &#x27;111.8.60.9:8123&#x27;, &#x27;user_pass&#x27;: &#x27;&#x27;&#125;,</span><br><span class="line">    &#123;&#x27;ip_port&#x27;: &#x27;101.71.27.120:80&#x27;, &#x27;user_pass&#x27;: &#x27;&#x27;&#125;,</span><br><span class="line">]                       #  代理  ，在downloder middleware 的 代理中间件中 proxy = random.choice(PROXIES)   request.meta[&#x27;proxy&#x27;] = &quot;http://%s&quot; % proxy[&#x27;ip_port&#x27;]</span><br><span class="line">SPIDER_MIDDLEWARES：爬虫中间层</span><br><span class="line">DOWNLOADER_MIDDLEWARES：下载中间层</span><br><span class="line">ITEM_PIPELINES = &#123;&#x27;项目名.pipelines.PipeLine类名&#x27;: 300,&#125;</span><br><span class="line"></span><br><span class="line"># 日志管理</span><br><span class="line">LOG_ENABLED 默认: True，启用logging</span><br><span class="line">LOG_ENCODING 默认: &#x27;utf-8&#x27;，logging使用的编码</span><br><span class="line">LOG_FILE 默认: None，在当前目录里创建logging输出文件的文件名，例如：LOG_FILE = &#x27;log.txt&#x27;</span><br><span class="line">    配置了这个文件，就不会在控制台输出日志了</span><br><span class="line">LOG_LEVEL 默认: &#x27;DEBUG&#x27;，log的最低级别，共五级：</span><br><span class="line">    CRITICAL - 严重错误</span><br><span class="line">    ERROR - 一般错误</span><br><span class="line">    WARNING - 警告信息</span><br><span class="line">    INFO - 一般信息  print 属于这个 info</span><br><span class="line">    DEBUG - 调试信息</span><br><span class="line">LOG_STDOUT 默认: False 如果为 True，进程所有的标准输出(及错误)将会被重定向到log中。</span><br><span class="line">        例如，执行 print(&quot;hello&quot;) ，其将会在Scrapy log中显示</span><br><span class="line"></span><br><span class="line"># 并发 ,现在 = 号右边的 value 就是默认值</span><br><span class="line">CONCURRENT_ITEMS = 100 #  并发处理 items 的最大数量</span><br><span class="line">CONCURRENT_REQUESTS = 16  #  并发下载request页面的最大数量</span><br><span class="line">CONCURRENT_REQUESTS_PER_DOMAIN = 8 # 并发下载任何单域的最大数量, baidu.com , sina.cn 各 8 个</span><br><span class="line">CONCURRENT_REQUESTS_PER_IP = 0 # 并发 每个IP 请求的最大数量，</span><br><span class="line">DOWNLOAD_DELAY = 0.25 # 单位秒，支持小数，一般都是随机范围：</span><br><span class="line">                        0.5*DOWNLOAD_DELAY 到 1.5*DOWNLOAD_DELAY 之间</span><br><span class="line">                        CONCURRENT_REQUESTS_PER_IP 不为0时，这个延时是针对每个IP，而不是每个域</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3><span id="2-爬虫类">2、爬虫类</span></h3><blockquote>
<p>继承自<strong>scrapy.Spider</strong>，用于构造Request对象给Scheduler</p>
<p>属性：</p>
<p>name：爬虫的名字，必须唯一 ，必须写！</p>
<p>start_urls：爬虫初始爬取的链接列表</p>
<p>custom_setting &#x3D; {} :</p>
<p><strong>如果这里配置了某个key，那么会覆盖掉 通用的项目settings中的配置项,自定义的setting配置</strong></p>
<p>方法：</p>
<p>start_requests：<strong>启动爬虫的时候调用，爬取urls的链接，可以省略</strong></p>
<p>parse：</p>
<p><strong>response到达spider的时候默认调用，如果在Request对象配置了callback函数，则不会调用，这个parse不能修改名称，parse方法可以迭代返回Item或Request对象，如果返回Request对象，则会进行增量爬取</strong></p>
</blockquote>
<h3><span id="3-items类">3、items类</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">复制# 继承自scrapy.Item</span><br><span class="line">定义：</span><br><span class="line">class Product(scrapy.Item):</span><br><span class="line">    name = scrapy.Field()</span><br><span class="line">    title = scrapy.Field()</span><br><span class="line"></span><br><span class="line">调用：</span><br><span class="line">和dict一样的调用</span><br><span class="line">product = Product(name=&#x27;Desktop PC&#x27;, title=&#x27;pc title&#x27;)</span><br><span class="line"># 像字典一样的使用：</span><br><span class="line">print(product[&#x27;name&#x27;])</span><br><span class="line">print(product.get(&#x27;name&#x27;))</span><br><span class="line">product[&#x27;title&#x27;] = &#x27;new title&#x27;</span><br><span class="line">可以这样转换为字典：dict(product)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3><span id="4-spider的-parse">4、spider的 parse</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">复制1、ItemLoader</span><br><span class="line"></span><br><span class="line">    def parse(self, response):</span><br><span class="line">        # text = response.text</span><br><span class="line">        # tree = etree.HTML(text)</span><br><span class="line">        # product = Product()</span><br><span class="line">        # product[&#x27;name&#x27;] = tree.xpath(&#x27;//div[@class=&quot;product_name&quot;]&#x27;)[0].text + 									tree.xpath(&#x27;//div[@class=&quot;product_title&quot;]&#x27;)[0].text</span><br><span class="line">        # product[&#x27;price&#x27;] = tree.xpath(&#x27;//p[@id=&quot;price&quot;]&#x27;)[0].text</span><br><span class="line">        # product[&#x27;stock&#x27;] = tree.xpath(&#x27;//p#stock&#x27;)[0].text</span><br><span class="line">        # product[&#x27;last_updated&#x27;] = &#x27;today&#x27;</span><br><span class="line">        # return product</span><br><span class="line">        l = ItemLoader(item=Product(), response=response)</span><br><span class="line">        l.add_xpath(&#x27;name&#x27;, &#x27;//div[@class=&quot;product_name&quot;]&#x27;)</span><br><span class="line">        l.add_xpath(&#x27;name&#x27;, &#x27;//div[@class=&quot;product_title&quot;]&#x27;)</span><br><span class="line">        l.add_xpath(&#x27;price&#x27;, &#x27;//p[@id=&quot;price&quot;]&#x27;)</span><br><span class="line">        l.add_css(&#x27;stock&#x27;, &#x27;p#stock&#x27;)</span><br><span class="line">        l.add_value(&#x27;last_updated&#x27;, &#x27;today&#x27;)  # you can also use literal values</span><br><span class="line">        return l.load_item()</span><br><span class="line"></span><br><span class="line"> 2、selector</span><br><span class="line"></span><br><span class="line">response.selector.xpath(&#x27;//span/text()&#x27;).extract()</span><br><span class="line"># 结果是 list，但是一般情况我们都是只取到一个值，如 [&#x27;想要的值&#x27;]， extract_first 取第一个</span><br><span class="line">response.css(&#x27;title::text&#x27;)</span><br><span class="line">response.css(&#x27;img&#x27;).xpath(&#x27;@src&#x27;).extract()</span><br><span class="line">response.xpath(&#x27;//div[@id=&quot;not-exists&quot;]/text()&#x27;).extract_first()</span><br><span class="line">response.xpath(&#x27;//div[@id=&quot;not-exists&quot;]/text()&#x27;).extract_first(default=&#x27;not-found&#x27;)</span><br><span class="line">response.xpath(&#x27;//a[contains(@href, &quot;image&quot;)]/text()&#x27;).re(r&#x27;Name:\s(.)&#x27;)</span><br><span class="line"> 3、meta</span><br><span class="line">yield Request(novel_url, self.parse_next, meta=&#123;&#x27;name&#x27;:name, &#x27;age&#x27;:age&#125;)</span><br><span class="line">在 parse_next 中 通过  response.meta[&#x27;name&#x27;] 获取参数</span><br><span class="line">传递参数到下一个 parse_next 函数</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3><span id="5-pipelines">5、pipelines</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br></pre></td><td class="code"><pre><span class="line">复制必须在settings中，添加</span><br><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">    &#x27;first_scrapy.pipelines.FirstScrapyPipeline&#x27;: 300, # 优先级，数字越小，</span><br><span class="line">                                                    优先级越高，越早调用范围 0-1000</span><br><span class="line">&#125;</span><br><span class="line">对象如下：</span><br><span class="line">class FirstScrapyPipeline(object):</span><br><span class="line">    def process_item(self, item, spider):</span><br><span class="line">        return item</span><br><span class="line"></span><br><span class="line">方法：</span><br><span class="line">	process_item(self, item, spider)： 处理item的方法， 必须有的！！！</span><br><span class="line">参数：</span><br><span class="line">	item (Item object or a dict) ： 获取到的item</span><br><span class="line">	spider (Spider object) ： 获取到item的spider</span><br><span class="line">返回:    一个dict或者item</span><br><span class="line"></span><br><span class="line">方法：</span><br><span class="line">    open_spider(self, spider) ： 当spider启动时，调用这个方法</span><br><span class="line">参数：</span><br><span class="line">	spider (Spider object) – 启动的spider</span><br><span class="line"></span><br><span class="line">方法：</span><br><span class="line">	close_spider(self, spider)： 当spider关闭时，调用这个方法</span><br><span class="line">参数：</span><br><span class="line">	spider (Spider object) – 关闭的spider</span><br><span class="line"></span><br><span class="line">@classmethod</span><br><span class="line">from_crawler(cls, crawler)</span><br><span class="line">参数：</span><br><span class="line">crawler (Crawler object) – 使用这个pipe的爬虫crawler</span><br><span class="line"></span><br><span class="line">4.1  返回 item 的例子：</span><br><span class="line">from scrapy.exceptions import DropItem</span><br><span class="line">class PricePipeline(object):</span><br><span class="line">    vat_factor = 1.15</span><br><span class="line">    def process_item(self, item, spider):</span><br><span class="line">        if item[&#x27;price&#x27;]:</span><br><span class="line">            if item[&#x27;price_excludes_vat&#x27;]:</span><br><span class="line">                item[&#x27;price&#x27;] = item[&#x27;price&#x27;] * self.vat_factor</span><br><span class="line">            return item</span><br><span class="line">        else:</span><br><span class="line">            raise DropItem(&quot;Missing price in %s&quot; % item)</span><br><span class="line"></span><br><span class="line">4.2 写入文件的例子：</span><br><span class="line">import json</span><br><span class="line">class JsonWriterPipeline(object):</span><br><span class="line">    def open_spider(self, spider):</span><br><span class="line">        self.file = open(&#x27;items.jl&#x27;, &#x27;w&#x27;)</span><br><span class="line">    def close_spider(self, spider):</span><br><span class="line">        self.file.close()</span><br><span class="line">    def process_item(self, item, spider):</span><br><span class="line">        line = json.dumps(dict(item)) + &quot;\n&quot;</span><br><span class="line">        self.file.write(line)</span><br><span class="line">        return item</span><br><span class="line"></span><br><span class="line">4.3 写入 mongodb 的例子：</span><br><span class="line">import pymongo</span><br><span class="line">class MongoPipeline(object):</span><br><span class="line">    collection_name = &#x27;scrapy_items&#x27;</span><br><span class="line">    def __init__(self, mongo_uri, mongo_db):</span><br><span class="line">        self.mongo_uri = mongo_uri</span><br><span class="line">        self.mongo_db = mongo_db</span><br><span class="line"></span><br><span class="line">    @classmethod</span><br><span class="line">    def from_crawler(cls, crawler):</span><br><span class="line">         #  必须在settings中 配置 MONGO_URI 和 MONGO_DATABASE</span><br><span class="line">        return cls(</span><br><span class="line">            mongo_uri=crawler.settings.get(&#x27;MONGO_URI&#x27;),</span><br><span class="line">            # items 是默认值，如果settings当中没有配置 MONGO_DATABASE ，那么 mongo_db = &#x27;items&#x27;</span><br><span class="line">            mongo_db=crawler.settings.get(&#x27;MONGO_DATABASE&#x27;, &#x27;items&#x27;)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    def open_spider(self, spider):</span><br><span class="line">        self.client = pymongo.MongoClient(self.mongo_uri)</span><br><span class="line">        self.db = self.client[self.mongo_db]</span><br><span class="line"></span><br><span class="line">    def close_spider(self, spider):</span><br><span class="line">        self.client.close()</span><br><span class="line"></span><br><span class="line">    def process_item(self, item, spider):</span><br><span class="line">        self.db[self.collection_name].insert_one(dict(item))</span><br><span class="line">        return item</span><br><span class="line"></span><br><span class="line">4.4  网页快照的例子</span><br><span class="line">import scrapy</span><br><span class="line">import hashlib</span><br><span class="line">from urllib.parse import quote</span><br><span class="line"></span><br><span class="line">class ScreenshotPipeline(object):</span><br><span class="line">    &quot;&quot;&quot;Pipeline that uses Splash to render screenshot of</span><br><span class="line">    every Scrapy item.&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    SPLASH_URL = &quot;http://localhost:8050/render.png?url=&#123;&#125;&quot;</span><br><span class="line"></span><br><span class="line">    def process_item(self, item, spider):</span><br><span class="line">        encoded_item_url = quote(item[&quot;url&quot;])</span><br><span class="line">        screenshot_url = self.SPLASH_URL.format(encoded_item_url)</span><br><span class="line">        request = scrapy.Request(screenshot_url)</span><br><span class="line">        dfd = spider.crawler.engine.download(request, spider)</span><br><span class="line">        dfd.addBoth(self.return_item, item)</span><br><span class="line">        return dfd</span><br><span class="line"></span><br><span class="line">    def return_item(self, response, item):</span><br><span class="line">        if response.status != 200:</span><br><span class="line">            # Error happened, return item.</span><br><span class="line">            return item</span><br><span class="line"></span><br><span class="line">        # Save screenshot to file, filename will be hash of url.</span><br><span class="line">        url = item[&quot;url&quot;]</span><br><span class="line">        url_hash = hashlib.md5(url.encode(&quot;utf8&quot;)).hexdigest()</span><br><span class="line">        filename = &quot;&#123;&#125;.png&quot;.format(url_hash)</span><br><span class="line">        with open(filename, &quot;wb&quot;) as f:</span><br><span class="line">            f.write(response.body)</span><br><span class="line"></span><br><span class="line">        # Store filename in item.</span><br><span class="line">        item[&quot;screenshot_filename&quot;] = filename</span><br><span class="line">        return item</span><br><span class="line"></span><br><span class="line">4.5 去重复值的pipe</span><br><span class="line">from scrapy.exceptions import DropItem</span><br><span class="line">class DuplicatesPipeline(object):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        self.ids_seen = set()</span><br><span class="line"></span><br><span class="line">    def process_item(self, item, spider):</span><br><span class="line">        if item[&#x27;id&#x27;] in self.ids_seen:</span><br><span class="line">            raise DropItem(&quot;Duplicate item found: %s&quot; % item)</span><br><span class="line">        else:</span><br><span class="line">            self.ids_seen.add(item[&#x27;id&#x27;])</span><br><span class="line">            return item</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3><span id="6-运行">6、运行</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">复制# 方法1、命令行中运行：</span><br><span class="line">命令行 中 进入到 first_scrapy 目录中，执行：</span><br><span class="line">scrapy crawl quotes</span><br><span class="line"></span><br><span class="line"># 方法2、pycharm 运行</span><br><span class="line">在 项目 根目录 添加 run.py 文件：</span><br><span class="line">from first_scrapy.spiders.quotes import QuotesSpider</span><br><span class="line">from scrapy.crawler import CrawlerProcess</span><br><span class="line">from scrapy.utils.project import get_project_settings</span><br><span class="line"># 获取settings.py模块的设置</span><br><span class="line">settings = get_project_settings()</span><br><span class="line">process = CrawlerProcess(settings=settings)</span><br><span class="line"># 可以添加多个spider</span><br><span class="line"># process.crawl(Spider1)</span><br><span class="line"># process.crawl(Spider2)</span><br><span class="line">process.crawl(QuotesSpider)</span><br><span class="line"># 启动爬虫，会阻塞，直到爬取完成</span><br><span class="line">process.start()</span><br><span class="line"></span><br><span class="line"># 方法三 、</span><br><span class="line">from scrapy.cmdline import execute</span><br><span class="line">#设置工程命令</span><br><span class="line">import sys</span><br><span class="line">import os</span><br><span class="line">#设置工程路径，在cmd 命令更改路径而执行scrapy命令调试</span><br><span class="line">#获取run文件的父目录，os.path.abspath(__file__) 为__file__文件目录</span><br><span class="line">sys.path.append(os.path.dirname(os.path.abspath(__file__)))</span><br><span class="line">execute([&quot;scrapy&quot;,&quot;crawl&quot;,&quot;quotes&quot; ])</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2><span id="四-案列">四、案列</span></h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br></pre></td><td class="code"><pre><span class="line">复制1、在命令行中，切换到 项目目录：譬如，f:\py_study&gt;</span><br><span class="line">   # 执行命令：scrapy startproject first_scrapy</span><br><span class="line">   # 将在 f:\py_study 路径下建立一个 first_scrapy 项目文件夹</span><br><span class="line">   # 文件夹结构如下：</span><br><span class="line">	scrapy.cfg            # 部署的配置文件，不需要修改</span><br><span class="line">    first_scrapy/</span><br><span class="line">        __init__.py</span><br><span class="line">        items.py   # 类一定要继承scrapy.Item，定义我们需要的结构化数据，和ORM有点类似，使用相当于dict</span><br><span class="line">      # </span><br><span class="line">      first_item = FirstscrapyItem()</span><br><span class="line">      name = first_item[&#x27;name&#x27;]</span><br><span class="line">      name1 = first_item.get(&#x27;name&#x27;)</span><br><span class="line">      first_item[&#x27;name&#x27;]  = &#x27;lucy&#x27;</span><br><span class="line"></span><br><span class="line">      # 可以转换为字典</span><br><span class="line">      dict_item = dict(first_item)</span><br><span class="line">      # 格式是： dict_item = &#123;name:&#x27;terry&#x27;, age:10, sex:&#x27;1&#x27;&#125;</span><br><span class="line"></span><br><span class="line">        middlewares.py    # 中间件，相当于钩子，可以对爬取前后做预处理，如修改请求header，url过滤等，分 downloadermiddleware、spidermiddleware</span><br><span class="line">        pipelines.py      # 数据处理，将items中结构化的数据进行处理</span><br><span class="line">        settings.py       # 项目配置文件，key=value 的方式，key必须全部大写！</span><br><span class="line">                          # 包括所有的配置，一些公用的常量也可以写在里面</span><br><span class="line">        spiders/          # 爬虫模块的目录，负责配置需要爬取的数据和爬取规则，以及解析数据，</span><br><span class="line">                          # 并且把结构化数据，return 到 pipelines 模块处理</span><br><span class="line">            __init__.py</span><br><span class="line"></span><br><span class="line">2、在 spiders 中新建：</span><br><span class="line">quotes.py：</span><br><span class="line"></span><br><span class="line">import scrapy</span><br><span class="line"># 必须继承 scrapy.Spider</span><br><span class="line">class QuotesSpider(scrapy.Spider):</span><br><span class="line">    # 用于区别Spider。 该名字必须是唯一的，您不可以为不同的Spider设定相同的名字。</span><br><span class="line">    name = &quot;quotes&quot;</span><br><span class="line"></span><br><span class="line">    def start_requests(self):</span><br><span class="line">        # 包含了Spider在启动时进行爬取的url列表。 因此，第一个被获取到的页面将是其中之一。</span><br><span class="line">        # 后续的URL则从初始的URL获取到的数据中提取。</span><br><span class="line">        urls = [</span><br><span class="line">            &#x27;http://quotes.toscrape.com/page/1/&#x27;,</span><br><span class="line">            &#x27;http://quotes.toscrape.com/page/2/&#x27;,</span><br><span class="line">        ]</span><br><span class="line">        for url in urls:</span><br><span class="line">            # 必须使用 yield</span><br><span class="line">            # Request 是 scrapy 自定义的类</span><br><span class="line">            # callback， 获取到response 之后的 回调函数</span><br><span class="line">            yield scrapy.Request(url=url, callback=self.parse)</span><br><span class="line"></span><br><span class="line">    # 是spider的一个方法。 被调用时，每个初始URL完成下载后生成的 Response 对象将会作为唯一的参数传递给该函数。</span><br><span class="line">    def parse(self, response):</span><br><span class="line">        page = response.url.split(&quot;/&quot;)[-2]</span><br><span class="line">        filename = &#x27;quotes-%s.html&#x27; % page</span><br><span class="line">        with open(filename, &#x27;wb&#x27;) as f:</span><br><span class="line">            f.write(response.body)</span><br><span class="line">        self.log(&#x27;Saved file %s&#x27; % filename)</span><br><span class="line"></span><br><span class="line">或</span><br><span class="line"></span><br><span class="line">class BlogSpider(scrapy.Spider):</span><br><span class="line">    name = &#x27;blogspider&#x27;</span><br><span class="line">    # 允许访问的域名，可以不写</span><br><span class="line">    allowed_domains = [&#x27;scrapinghub.com&#x27;]</span><br><span class="line">    start_urls = [&#x27;https://blog.scrapinghub.com&#x27;]</span><br><span class="line"></span><br><span class="line">    def parse(self, response):</span><br><span class="line">        for title in response.css(&#x27;h2.entry-title&#x27;):</span><br><span class="line">            yield &#123;&#x27;title&#x27;: title.css(&#x27;a ::text&#x27;).extract_first()&#125;</span><br><span class="line"></span><br><span class="line">        for next_page in response.css(&#x27;div.prev-post &gt; a&#x27;):</span><br><span class="line">            yield response.follow(next_page, self.parse)</span><br><span class="line"></span><br><span class="line">&#x27;&#x27;&#x27;</span><br><span class="line">补充：</span><br><span class="line">数据流，所有中间件都启动：</span><br><span class="line">1、启动 spiders</span><br><span class="line">2、spiders 包装一个request， 发送到 scheduler</span><br><span class="line">    2.1 request进入 scheduler 之前，会先到 scheduler middleware 进行处理，</span><br><span class="line">        处理后，再发送给 scheduler</span><br><span class="line">3、scheduler 接收到 request 后，将之放入到一个 队列</span><br><span class="line">4、engine 向 scheduler  申请 request，得到后，将request</span><br><span class="line">    发送给 downloader</span><br><span class="line">    4.1 request 从 scheduler 出来后，先到 scheduler middeware 进行处理，</span><br><span class="line">        再传给 scheduler middeware</span><br><span class="line">    4.2 request 从 scheduler middeware 出来后，需要到 downloader middleware</span><br><span class="line">        进行处理，再传给 downloader</span><br><span class="line">5、downloader 接收到 request 之后，访问对应的http资源，接收 response</span><br><span class="line">6、downloader 接收到 response 之后，将 response 发送给 spider</span><br><span class="line">    6.1 response 从 downloader 出来后，先经过 downloader middleware 处理</span><br><span class="line">    6.2 response 从 ownloader middleware 处理后，经过 spider middleware 处理，</span><br><span class="line">        再 传给 spider</span><br><span class="line">7、spider 接收到reponse之后，解析内容，</span><br><span class="line">    7.1  url 资源，需要再次请求的，继续包装成 request ，继续第二个步骤</span><br><span class="line">    7.2  非 url 资源，就是 数据，结构化成 item ，向后传</span><br><span class="line">8、item 从 spider 出来后，经过 spider middleware ，处理后 ，发送给</span><br><span class="line">   item pipeline</span><br><span class="line">9、pipeline 接收到 item 后，读取其中的数据，进行数据持久化</span><br><span class="line">&#x27;&#x27;&#x27;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h1><span id="十四-scrapy-redis学习">十四、scrapy-redis学习</span></h1><h2><span id="一-简介">一、简介</span></h2><blockquote>
<p>scrapy-redis是scrapy框架基于redis数据库的组件，用于scrapy项目的分布式开发和部署</p>
<p>特征：</p>
<ul>
<li>l 分布式爬取</li>
</ul>
<p>您可以启动多个spider工程，相互之间共享单个redis的requests队列。最适合广泛的多个域名网站的内容爬取。</p>
<ul>
<li>l 分布式数据处理</li>
</ul>
<p>爬取到的scrapy的item数据可以推入到redis队列中，这意味着你可以根据需求启动尽可能多的处理程序来共享item的队列，进行item数据持久化处理</p>
<ul>
<li>l Scrapy即插即用组件</li>
</ul>
<p>Scheduler调度器 + Duplication复制 过滤器，Item Pipeline，基本spider</p>
</blockquote>
<h2><span id="二-安装">二、安装</span></h2><blockquote>
<p>通过pip 安装： pip install scrapy-redis<br>本文档使用版本： 0.6.8<br>依赖环境：<br>python 3.6.3<br>redis 2.10.6<br>scrapy 1.5.0</p>
</blockquote>
<p><a href="http://xiaozhou.top/2021/05/18/redis%E5%AD%A6%E4%B9%A0/#more">redia详解</a></p>
<h2><span id="三-文档">三、文档</span></h2><blockquote>
<p>官方文档：<a target="_blank" rel="noopener" href="https://scrapy-redis.readthedocs.io/en/stable/">https://scrapy-redis.readthedocs.io/en/stable/</a></p>
<p>源码位置：<a target="_blank" rel="noopener" href="https://github.com/rmax/scrapy-redis">https://github.com/rmax/scrapy-redis</a></p>
</blockquote>
<h2><span id="四-配置文件">四、配置文件</span></h2><p><strong>配置文件是在 scrapy 的 settings 中进行修改的</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line">复制#启用Redis调度存储请求队列</span><br><span class="line">SCHEDULER = &quot;scrapy_redis.scheduler.Scheduler&quot;</span><br><span class="line"></span><br><span class="line">#确保所有的爬虫通过Redis去重</span><br><span class="line">DUPEFILTER_CLASS = &quot;scrapy_redis.dupefilter.RFPDupeFilter&quot;</span><br><span class="line"></span><br><span class="line">#默认请求序列化使用的是pickle 但是我们可以更改为其他类似的。该配置2.X的可以用。3.X的不能用</span><br><span class="line">#SCHEDULER_SERIALIZER = &quot;scrapy_redis.picklecompat&quot;</span><br><span class="line"></span><br><span class="line">#不清除Redis队列、这样可以暂停/恢复 爬取</span><br><span class="line"># SCHEDULER_PERSIST = True</span><br><span class="line"></span><br><span class="line">#使用优先级调度请求队列 （默认使用）</span><br><span class="line">#SCHEDULER_QUEUE_CLASS = &#x27;scrapy_redis.queue.PriorityQueue&#x27;</span><br><span class="line">#可选用的其它队列</span><br><span class="line">#SCHEDULER_QUEUE_CLASS = &#x27;scrapy_redis.queue.FifoQueue&#x27;</span><br><span class="line">#SCHEDULER_QUEUE_CLASS = &#x27;scrapy_redis.queue.LifoQueue&#x27;</span><br><span class="line"></span><br><span class="line">#最大空闲时间防止分布式爬虫因为等待而关闭</span><br><span class="line">#这只有当上面设置的队列类是SpiderQueue或SpiderStack时才有效</span><br><span class="line">#并且当您的蜘蛛首次启动时，也可能会阻止同一时间启动（由于队列为空）</span><br><span class="line">#SCHEDULER_IDLE_BEFORE_CLOSE = 10</span><br><span class="line"></span><br><span class="line">#将数据项目在redis进行处理</span><br><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">    &#x27;scrapy_redis.pipelines.RedisPipeline&#x27;: 300</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">#序列化项目管道作为redis Key存储，假如爬虫类的name=&#x27;first&#x27;，name这个KEY就是 &#x27;first:items&#x27;</span><br><span class="line">#REDIS_ITEMS_KEY = &#x27;%(spider)s:items&#x27;</span><br><span class="line"></span><br><span class="line">#默认使用ScrapyJSONEncoder进行项目序列化</span><br><span class="line">#You can use any importable path to a callable object.</span><br><span class="line">#REDIS_ITEMS_SERIALIZER = &#x27;json.dumps&#x27;</span><br><span class="line"></span><br><span class="line">#指定连接到redis时使用的端口和地址（可选）</span><br><span class="line">#REDIS_HOST = &#x27;localhost&#x27;</span><br><span class="line">#REDIS_PORT = 6379</span><br><span class="line"></span><br><span class="line">#指定用于连接redis的URL（可选）</span><br><span class="line">#如果设置此项，则此项优先级高于设置的REDIS_HOST 和 REDIS_PORT</span><br><span class="line">#REDIS_URL = &#x27;redis://user:pass@hostname:9001&#x27;</span><br><span class="line"></span><br><span class="line">#自定义的redis参数（连接超时之类的）</span><br><span class="line">#REDIS_PARAMS  = &#123;&#125;</span><br><span class="line"></span><br><span class="line">#自定义redis客户端类</span><br><span class="line">#REDIS_PARAMS[&#x27;redis_cls&#x27;] = &#x27;myproject.RedisClient&#x27;</span><br><span class="line"></span><br><span class="line">#如果为True，则使用redis的&#x27;spop&#x27;进行操作。</span><br><span class="line">#如果需要避免起始网址列表出现重复，这个选项非常有用。开启此选项urls必须通过sadd添加，否则会出现类型错误。</span><br><span class="line">#REDIS_START_URLS_AS_SET = False</span><br><span class="line"></span><br><span class="line">#RedisSpider和RedisCrawlSpider默认 start_urls 键</span><br><span class="line">#REDIS_START_URLS_KEY = &#x27;%(name)s:start_urls&#x27;</span><br><span class="line"></span><br><span class="line">#设置redis使用utf-8之外的编码</span><br><span class="line">#REDIS_ENCODING = &#x27;latin1&#x27;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><strong>最常用的配置修改为:</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">复制SCHEDULER = &quot;scrapy_redis.scheduler.Scheduler&quot;</span><br><span class="line"></span><br><span class="line">DUPEFILTER_CLASS = &quot;scrapy_redis.dupefilter.RFPDupeFilter&quot;</span><br><span class="line"></span><br><span class="line">REDIS_URL = &#x27;redis://root:密码@主机ＩＰ:端口&#x27;</span><br><span class="line"></span><br><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">    &#x27;scrapy_redis.pipelines.RedisPipeline&#x27;: 300</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">SCHEDULER_PERSIST = True</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><strong>以上常用配置下，会在 redis 中，增加3个key：</strong></p>
<blockquote>
<ul>
<li>1、 项目名:items</li>
</ul>
<p>list 类型，保存爬虫获取到的 数据item<br>内容是 json 字符串</p>
<ul>
<li>2、 项目名:dupefilter</li>
</ul>
<p>set类型，用于爬虫访问的URL去重<br>内容是 40个字符的 url 的hash 字符串zme</p>
<ul>
<li>3、 项目名: start_urls (这个一般用不到)</li>
</ul>
<p>List 类型，用于获取spider启动时爬取的第一个url</p>
<ul>
<li>4、 项目名:requests</li>
</ul>
<p>zset类型，用于scheduler调度处理 requests<br>内容是 request 对象的序列化 字符串</p>
<p>scrapy-redis对scrapy项目的改造，只需要增加以上几个配置就可以完成了</p>
</blockquote>
<p>[^abc]:</p>

      </section>

      
      
        <nav class="article-nav">
          
            <div class="article-nav-item layout-padding">
  <article class="card-container article-nav-card content-padding--primary soft-size--large soft-style--box">
    
    <div class="card-text">
      
        <a href="/2024/10/04/Docker%E8%AF%A6%E8%A7%A3/" itemprop="url">
          <h2 class="card-text--title text-ellipsis">Docker详解</h2>
        </a>
      
      <div class="card-text--row">Newer</div>
    </div>
  </article>
</div>
          
          
            <div class="article-nav-item layout-padding">
  <article class="card-container article-nav-card content-padding--primary soft-size--large soft-style--box">
    
    <div class="card-text">
      
        <a href="/2024/10/04/flex%E5%B8%83%E5%B1%80/" itemprop="url">
          <h2 class="card-text--title text-ellipsis">flex布局</h2>
        </a>
      
      <div class="card-text--row">Older</div>
    </div>
  </article>
</div>
          
        </nav>
      

      <section class="page-message-container layout-padding">
        


  
  

  
  


      </section>
    </div>
    <div class="widget-info">
      <section class="widget-author widget-item layout-margin content-padding--primary soft-size--large soft-style--box">
  <div class="widget-body">
    
      <img src="https://img2.baidu.com/it/u=3940764199,1352508461&amp;fm=253&amp;fmt=auto&amp;app=138&amp;f=JPEG?w=800&amp;h=800" class="soft-size--round soft-style--box" alt="ChengZhou">
    
    
      <h2>ChengZhou</h2>
    
    
      <p>贵在坚持</p>
    

    <div class="count-box">
      <div class="count-box--item">
        <svg class="icon icon-article" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg">
  <path d="M240.51564747 647.74217627h196.07203239c16.59071043 0 30.16492806-13.57421762 30.16492805-30.16492806V165.10332731c0-33.18142087-30.16492806-60.32985613-60.32985612-60.32985611H245.04038668C225.43318342 104.7734712 210.35071939 119.85593522 210.35071939 139.46313845V617.57724821c0 16.59071043 13.57421762 30.16492806 30.16492808 30.16492806z m663.62841731-452.47392089v482.63884894c0 33.18142087-27.14843525 60.32985613-60.32985612 60.32985613H180.18579134c-33.18142087 0-60.32985613-27.14843525-60.32985612-60.32985613V195.26825538c-49.77213131 0-90.49478418 40.72265287-90.49478417 90.49478417v452.4739209c0 49.77213131 40.72265287 90.49478418 90.49478417 90.49478417h286.56681657c16.59071043 0 30.16492806 13.57421762 30.16492807 30.16492807s13.57421762 30.16492806 30.16492805 30.16492806h90.49478418c16.59071043 0 30.16492806-13.57421762 30.16492805-30.16492806s13.57421762-30.16492806 30.16492807-30.16492807h286.56681657c49.77213131 0 90.49478418-40.72265287 90.49478417-90.49478417V285.76303955c0-49.77213131-40.72265287-90.49478418-90.49478417-90.49478417zM587.41232014 647.74217627h191.54729318c19.60720323 0 34.68966726-15.08246403 34.68966729-34.68966727V134.93839925c0-16.59071043-13.57421762-30.16492806-30.16492808-30.16492805H617.57724821c-30.16492806 0-60.32985613 27.14843525-60.32985612 60.32985611v452.4739209c0 16.59071043 13.57421762 30.16492806 30.16492805 30.16492806z" fill="currentColor"></path>
</svg>
        <span>20</span>
      </div>
      <div class="count-box--item">
        <svg class="icon icon-categories" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg">
  <path d="M900.3614811 257.09082106h-339.81629553l-67.96326003-101.9448889c-19.41807444-29.12711113-48.54518557-43.69066667-82.52681443-43.69066667H123.6385189c-53.39970333 0-97.09036999 43.69066667-97.09037113 97.09036999v582.54222222c0 53.39970333 43.69066667 97.09036999 97.09037113 97.09037002h776.7229622c53.39970333 0 97.09036999-43.69066667 97.09037113-97.09037002V354.18119104c0-53.39970333-43.69066667-97.09036999-97.09037113-97.09036998z m-97.09036999 242.72592554H220.72888889c-24.27259221 0-48.54518557-24.27259221-48.54518556-48.54518556s24.27259221-48.54518557 48.54518556-48.54518444h582.54222222c24.27259221 0 48.54518557 24.27259221 48.54518556 48.54518444s-24.27259221 48.54518557-48.54518556 48.54518556z" fill="currentColor"></path>
</svg>
        0
      </div>
      <div class="count-box--item">
        <svg class="icon icon-tags" viewBox="0 0 1098 1024" version="1.1" xmlns="http://www.w3.org/2000/svg">
  <path d="M283.42180005 272q0-28.38857157-20.09142843-48.48000001t-48.47999998-20.09142842-48.48000002 20.09142842-20.09142846 48.48000001 20.09142846 48.48 48.48000002 20.09142843 48.47999998-20.09142843 20.09142843-48.48zM855.0332285 580.57142843q0 28.38857157-19.81714313 48.2057147l-263.03999997 263.58857157q-20.9142853 19.81714313-48.75428534 19.81714312-28.38857157 0-48.20571468-19.81714312l-383.04-383.58857157q-20.36571468-19.81714313-34.55999999-54.10285688t-14.19428534-62.6742853l0-222.85714313q0-27.84000002 20.36571469-48.20571469t48.2057147-20.36571466l222.85714313 0q28.38857157 0 62.6742853 14.19428529t54.65142842 34.55999999l383.04000001 382.49142843q19.81714313 20.9142853 19.81714314 48.75428532zM1060.74751475 580.57142843q0 28.38857157-19.81714313 48.2057147l-263.04 263.58857157q-20.9142853 19.81714313-48.75428531 19.81714312-19.26857155 0-31.61142843-7.47428531t-28.38857159-24.13714314l251.79428534-251.7942853q19.81714313-19.81714313 19.81714308-48.20571469 0-27.84000002-19.81714308-48.75428531l-383.04000001-382.49142845q-20.36571468-20.36571468-54.65142842-34.55999999t-62.67428532-14.19428534l120 0q28.38857157 0 62.67428532 14.19428534t54.65142842 34.55999999l383.03999998 382.49142845q19.81714313 20.9142853 19.81714314 48.75428531z" fill="currentColor"></path>
</svg>
        17
      </div>
    </div>
  </div>
</section>

      

      
<section class="widet-notice widget-item layout-margin content-padding--primary soft-size--large soft-style--box">
  <div class="widget-title">
    <svg class="icon icon-notice" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg">
  <path d="M512 945.02305225v28.15620663a24.27259221 24.27259221 0 0 1-24.27259221 24.27259335H394.0352a48.54518557 48.54518557 0 0 1-41.74885888-23.78714112l-110.68302222-184.47170332a132.04290333 132.04290333 0 0 1-17.47626667-48.54518557h118.4502511a200.97706667 200.97706667 0 0 1 76.21594113 14.56355556l20.38897777 133.49925888a48.54518557 48.54518557 0 0 0 36.40888888 27.67075555l16.01991111 2.91271112a24.27259221 24.27259221 0 0 1 20.38897778 25.72894889zM997.45185223 463.45481443a194.18074112 194.18074112 0 0 1-38.8361489 116.50844445 24.75804445 24.75804445 0 0 1-36.4088889 0l-34.95253333-34.95253333a24.27259221 24.27259221 0 0 1-2.91271111-30.58346667 97.09036999 97.09036999 0 0 0 0-106.79940665 24.27259221 24.27259221 0 0 1 2.91271111-30.58346666l34.95253333-34.95253334a24.75804445 24.75804445 0 0 1 18.93262223-7.28177777 26.2144 26.2144 0 0 1 17.47626667 9.70903665A194.18074112 194.18074112 0 0 1 997.45185223 463.45481443z m-194.18074112-388.36148111v776.72296335a48.54518557 48.54518557 0 0 1-48.54518556 48.54518443h-28.64165888a48.54518557 48.54518557 0 0 1-33.98163001-14.07810332l-145.63555556-143.20829668A291.27111111 291.27111111 0 0 0 342.57730333 657.63555556H172.18370333a145.63555556 145.63555556 0 0 1-145.63555556-145.63555556v-97.09036999a145.63555556 145.63555556 0 0 1 145.63555556-145.63555556h170.3936a291.27111111 291.27111111 0 0 0 206.31703779-85.43952668l145.63555555-143.20829554a48.54518557 48.54518557 0 0 1 33.98162888-14.07810446H754.72592555a48.54518557 48.54518557 0 0 1 48.54518556 48.54518555z" fill="currentColor"></path>
</svg>
    <span>NOTICE</span>
  </div>
  <div class="widget-body">
    <p>学习中。。。</p>
  </div>
</section>


      <section class="widget-categories widget-item layout-margin content-padding--primary soft-size--large soft-style--box">
  <div class="widget-title">
    <svg class="icon icon-categories" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg">
  <path d="M900.3614811 257.09082106h-339.81629553l-67.96326003-101.9448889c-19.41807444-29.12711113-48.54518557-43.69066667-82.52681443-43.69066667H123.6385189c-53.39970333 0-97.09036999 43.69066667-97.09037113 97.09036999v582.54222222c0 53.39970333 43.69066667 97.09036999 97.09037113 97.09037002h776.7229622c53.39970333 0 97.09036999-43.69066667 97.09037113-97.09037002V354.18119104c0-53.39970333-43.69066667-97.09036999-97.09037113-97.09036998z m-97.09036999 242.72592554H220.72888889c-24.27259221 0-48.54518557-24.27259221-48.54518556-48.54518556s24.27259221-48.54518557 48.54518556-48.54518444h582.54222222c24.27259221 0 48.54518557 24.27259221 48.54518556 48.54518444s-24.27259221 48.54518557-48.54518556 48.54518556z" fill="currentColor"></path>
</svg>
      <span>CATEGORIES</span>
  </div>
  <div class="widget-body">
    <ul class="categories-list">
      
    </ul>
  </div>
</section>

      <section class="widget-tags widget-item  layout-margin content-padding--primary soft-size--large soft-style--box">
  <div class="widget-title">
    <svg class="icon icon-tags" viewBox="0 0 1098 1024" version="1.1" xmlns="http://www.w3.org/2000/svg">
  <path d="M283.42180005 272q0-28.38857157-20.09142843-48.48000001t-48.47999998-20.09142842-48.48000002 20.09142842-20.09142846 48.48000001 20.09142846 48.48 48.48000002 20.09142843 48.47999998-20.09142843 20.09142843-48.48zM855.0332285 580.57142843q0 28.38857157-19.81714313 48.2057147l-263.03999997 263.58857157q-20.9142853 19.81714313-48.75428534 19.81714312-28.38857157 0-48.20571468-19.81714312l-383.04-383.58857157q-20.36571468-19.81714313-34.55999999-54.10285688t-14.19428534-62.6742853l0-222.85714313q0-27.84000002 20.36571469-48.20571469t48.2057147-20.36571466l222.85714313 0q28.38857157 0 62.6742853 14.19428529t54.65142842 34.55999999l383.04000001 382.49142843q19.81714313 20.9142853 19.81714314 48.75428532zM1060.74751475 580.57142843q0 28.38857157-19.81714313 48.2057147l-263.04 263.58857157q-20.9142853 19.81714313-48.75428531 19.81714312-19.26857155 0-31.61142843-7.47428531t-28.38857159-24.13714314l251.79428534-251.7942853q19.81714313-19.81714313 19.81714308-48.20571469 0-27.84000002-19.81714308-48.75428531l-383.04000001-382.49142845q-20.36571468-20.36571468-54.65142842-34.55999999t-62.67428532-14.19428534l120 0q28.38857157 0 62.67428532 14.19428534t54.65142842 34.55999999l383.03999998 382.49142845q19.81714313 20.9142853 19.81714314 48.75428531z" fill="currentColor"></path>
</svg>
    <span>TAGS</span>
  </div>
  <div class="widget-body">
    <div class="tags-cloud">
      <a href="/tags/Django/" style="font-size: 10px;" class="tags-cloud-0">Django</a> <a href="/tags/Django-Linux/" style="font-size: 10px;" class="tags-cloud-0">Django Linux</a> <a href="/tags/Excel%E8%87%AA%E5%8A%A8%E5%8C%96/" style="font-size: 10px;" class="tags-cloud-0">Excel自动化</a> <a href="/tags/Flask/" style="font-size: 10px;" class="tags-cloud-0">Flask</a> <a href="/tags/Linux-Mysql/" style="font-size: 10px;" class="tags-cloud-0">Linux Mysql</a> <a href="/tags/Python/" style="font-size: 10px;" class="tags-cloud-0">Python</a> <a href="/tags/Python-Mysql/" style="font-size: 10px;" class="tags-cloud-0">Python Mysql</a> <a href="/tags/Redis/" style="font-size: 10px;" class="tags-cloud-0">Redis</a> <a href="/tags/TkGUI/" style="font-size: 10px;" class="tags-cloud-0">TkGUI</a> <a href="/tags/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/" style="font-size: 10px;" class="tags-cloud-0">博客搭建</a> <a href="/tags/%E5%AE%B9%E5%99%A8%E9%83%A8%E7%BD%B2/" style="font-size: 20px;" class="tags-cloud-10">容器部署</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/" style="font-size: 10px;" class="tags-cloud-0">数据结构</a> <a href="/tags/%E6%B5%8F%E8%A7%88%E5%99%A8%E8%87%AA%E5%8A%A8%E5%8C%96/" style="font-size: 10px;" class="tags-cloud-0">浏览器自动化</a> <a href="/tags/%E7%88%AC%E8%99%AB/" style="font-size: 10px;" class="tags-cloud-0">爬虫</a> <a href="/tags/%E7%BD%91%E9%A1%B5%E5%B8%83%E5%B1%80/" style="font-size: 10px;" class="tags-cloud-0">网页布局</a> <a href="/tags/%E8%87%AA%E5%8A%A8%E5%8C%96/" style="font-size: 15px;" class="tags-cloud-5">自动化</a> <a href="/tags/%E9%A1%B9%E7%9B%AE%E8%AE%B0%E5%BD%95/" style="font-size: 10px;" class="tags-cloud-0">项目记录</a>
    </div>
  </div>
</section>
    </div>
  </article>
</div>

    <!-- footer container -->
<footer id="footer" class="footer">
  <div class="footer-container">
    
    <div class="social-icons">
      
        
      
        
      
        
      
        
          <a href="https://github.com/Xiaozhou-h" class="soft-size--primary soft-style--box" target="_blank" rel="noopener noreferrer">
            <svg class="icon icon-github" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg">
  <path d="M64.6 512c0 195.6 125.4 361.9 300.1 422.9 23.5 5.9 19.9-10.8 19.9-22.2v-77.6c-135.8 15.9-141.3-74-150.5-89-18.5-31.5-61.9-39.5-49-54.5 31-15.9 62.5 4 98.9 58 26.4 39.1 77.9 32.5 104.1 26 5.7-23.5 17.9-44.5 34.7-60.9-140.7-25.2-199.4-111.1-199.4-213.3 0-49.5 16.4-95.1 48.4-131.8-20.4-60.6 1.9-112.4 4.9-120.1 58.2-5.2 118.5 41.6 123.3 45.3 33.1-8.9 70.8-13.7 112.9-13.7 42.4 0 80.3 4.9 113.5 13.9 11.3-8.6 67.3-48.8 121.4-43.9 2.9 7.7 24.7 58.3 5.5 118.1 32.5 36.8 49 82.8 49 132.4 0 102.3-59 188.3-200.2 213.2 23.5 23.3 38.1 55.5 38.1 91.1v112.7c0.8 9 0 17.9 15.1 17.9C832.7 877 960.4 709.4 960.4 512.1c0-247.5-200.6-447.9-447.9-447.9C265 64.1 64.6 264.5 64.6 512z"></path>
</svg>
          </a>
        
      
        
          <a href="https://github.com/Xiaozhou-h" class="soft-size--primary soft-style--box" target="_blank" rel="noopener noreferrer">
            <svg class="icon icon-twitter" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg">
  <path d="M962.285714 233.142857q-38.285714 56-92.571429 95.428571 0.571429 8 0.571429 24 0 74.285714-21.714286 148.285714t-66 142-105.428571 120.285714-147.428571 83.428571-184.571429 31.142857q-154.857143 0-283.428571-82.857143 20 2.285714 44.571429 2.285714 128.571429 0 229.142857-78.857143-60-1.142857-107.428571-36.857143t-65.142857-91.142857q18.857143 2.857143 34.857143 2.857143 24.571429 0 48.571429-6.285714-64-13.142857-106-63.714286t-42-117.428571l0-2.285714q38.857143 21.714286 83.428571 23.428571-37.714286-25.142857-60-65.714286t-22.285714-88q0-50.285714 25.142857-93.142857 69.142857 85.142857 168.285714 136.285714t212.285714 56.857143q-4.571429-21.714286-4.571429-42.285714 0-76.571429 54-130.571429t130.571429-54q80 0 134.857143 58.285714 62.285714-12 117.142857-44.571429-21.142857 65.714286-81.142857 101.714286 53.142857-5.714286 106.285714-28.571429z"></path>
</svg>
          </a>
        
      
    </div>
     
    <p>&copy; 2024 <a href="/" target="_blank">ChengZhou</a></p>

    

    <p>Powered by <a href="https://hexo.io" target="_blank" rel="noopener noreferrer">Hexo</a> Theme - <a href="https://github.com/miiiku/flex-block" target="_blank" rel="noopener noreferrer author">flex-block</a></p>

    <p>
      <a href="javascript:;" id="theme-light">🌞 浅色</a>
      <a href="javascript:;" id="theme-dark">🌛 深色</a>
      <a href="javascript:;" id="theme-auto">🤖️ 自动</a>
    </p>
  </div>
</footer>
  </div>

  <div class="back-to-top-fixed soft-size--round soft-style--box">
    <svg class="icon icon-back-to-top" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg">
      <path d="M725.333333 426.666667c-12.8 0-21.333333-4.266667-29.866667-12.8l-213.333333-213.333333c-17.066667-17.066667-17.066667-42.666667 0-59.733333s42.666667-17.066667 59.733333 0l213.333333 213.333333c17.066667 17.066667 17.066667 42.666667 0 59.733333C746.666667 422.4 738.133333 426.666667 725.333333 426.666667z"></path>
      <path d="M298.666667 426.666667c-12.8 0-21.333333-4.266667-29.866667-12.8-17.066667-17.066667-17.066667-42.666667 0-59.733333l213.333333-213.333333c17.066667-17.066667 42.666667-17.066667 59.733333 0s17.066667 42.666667 0 59.733333l-213.333333 213.333333C320 422.4 311.466667 426.666667 298.666667 426.666667z"></path>
      <path d="M512 896c-25.6 0-42.666667-17.066667-42.666667-42.666667L469.333333 170.666667c0-25.6 17.066667-42.666667 42.666667-42.666667s42.666667 17.066667 42.666667 42.666667l0 682.666667C554.666667 878.933333 537.6 896 512 896z"></path>
    </svg>
  </div>

  
  






<!-- copy button  -->
<script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js"></script>

<!-- https://clipboardjs.com/ -->










  


  


  




<script src="/js/script.js"></script>


  
  <!-- 尾部用户自定义相关内容 -->
</body>
</html>
